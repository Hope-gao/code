{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11283132,"sourceType":"datasetVersion","datasetId":7054451},{"sourceId":11283895,"sourceType":"datasetVersion","datasetId":7054329}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T02:51:22.944578Z","iopub.execute_input":"2025-04-06T02:51:22.944861Z","iopub.status.idle":"2025-04-06T02:51:24.048261Z","shell.execute_reply.started":"2025-04-06T02:51:22.944828Z","shell.execute_reply":"2025-04-06T02:51:24.047270Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nq10k-comp5423/New_metrics_calculation.py\n/kaggle/input/nq10k-comp5423/data_and_code/metrics_calculation.py\n/kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n/kaggle/input/nq10k-comp5423/data_and_code/data/test.jsonl\n/kaggle/input/nq10k-comp5423/data_and_code/data/(example) test_predict.jsonl\n/kaggle/input/nq10k-comp5423/data_and_code/data/val_predict.jsonl\n/kaggle/input/nq10k-comp5423/data_and_code/data/train.jsonl\n/kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n/kaggle/input/crawl-300d-2m/crawl-300d-2M.vec\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport os\n\n# --- 配置 ---\n# 定义数据文件所在的目录路径\n# 请确保 'data' 文件夹与此脚本在同一目录下，或者提供绝对路径\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data/'\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nTRAIN_FILE = os.path.join(DATA_DIR, 'train.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl')\nTEST_FILE = os.path.join(DATA_DIR, 'test.jsonl')\n\n# --- 函数：加载 JSON Lines 文件 ---\ndef load_jsonl(file_path):\n    \"\"\"\n    加载 JSON Lines 文件 (.jsonl) 并返回数据列表。\n    每个元素是一个从 JSON 行解析出的 Python 字典。\n\n    Args:\n        file_path (str): .jsonl 文件的路径。\n\n    Returns:\n        list[dict]: 包含文件中所有 JSON 对象的列表。\n                    如果文件不存在或为空，则返回空列表。\n    \"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                if line.strip(): # 确保行不为空\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳过无法解析的行: {line.strip()} - 错误: {e}\")\n        print(f\"成功加载 {len(data)} 条记录从 {file_path}\")\n    except Exception as e:\n        print(f\"加载文件时出错 {file_path}: {e}\")\n    return data\n\n# --- 函数：将列表数据转换为 Pandas DataFrame ---\ndef list_to_dataframe(data_list):\n    \"\"\"\n    将字典列表转换为 Pandas DataFrame。\n\n    Args:\n        data_list (list[dict]): 字典列表。\n\n    Returns:\n        pd.DataFrame: 转换后的 DataFrame。如果列表为空，则返回空的 DataFrame。\n    \"\"\"\n    if not data_list:\n        return pd.DataFrame()\n    return pd.DataFrame(data_list)\n\n# --- 主程序 ---\nif __name__ == \"__main__\":\n    print(\"开始加载 NQ10K 数据集...\")\n\n    # 加载文档数据\n    print(f\"\\n--- 加载文档 ---\")\n    documents_data = load_jsonl(DOCUMENTS_FILE)\n    documents_df = list_to_dataframe(documents_data)\n    if not documents_df.empty:\n        print(f\"文档数据预览 (前 5 条):\")\n        print(documents_df.head())\n        print(f\"\\n文档总数: {len(documents_df)}\")\n        print(f\"列名: {documents_df.columns.tolist()}\")\n        # 检查是否有缺失值\n        print(\"\\n文档数据缺失值检查:\")\n        print(documents_df.isnull().sum())\n        # 将文档存储在字典中，方便后续按 ID 查找\n        documents_dict = {doc['document_id']: doc['document_text'] for doc in documents_data if 'document_id' in doc and 'document_text' in doc}\n        print(f\"已将 {len(documents_dict)} 个文档存入字典。\")\n    else:\n        print(\"未能加载文档数据或文档数据为空。\")\n        documents_dict = {}\n\n    # 加载训练数据\n    print(f\"\\n--- 加载训练数据 ---\")\n    train_data = load_jsonl(TRAIN_FILE)\n    train_df = list_to_dataframe(train_data)\n    if not train_df.empty:\n        print(f\"训练数据预览 (前 5 条):\")\n        print(train_df.head())\n        print(f\"\\n训练样本总数: {len(train_df)}\")\n        print(f\"列名: {train_df.columns.tolist()}\")\n        print(\"\\n训练数据缺失值检查:\")\n        print(train_df.isnull().sum())\n    else:\n        print(\"未能加载训练数据或训练数据为空。\")\n\n    # 加载验证数据\n    print(f\"\\n--- 加载验证数据 ---\")\n    val_data = load_jsonl(VAL_FILE)\n    val_df = list_to_dataframe(val_data)\n    if not val_df.empty:\n        print(f\"验证数据预览 (前 5 条):\")\n        print(val_df.head())\n        print(f\"\\n验证样本总数: {len(val_df)}\")\n        print(f\"列名: {val_df.columns.tolist()}\")\n        print(\"\\n验证数据缺失值检查:\")\n        print(val_df.isnull().sum())\n    else:\n        print(\"未能加载验证数据或验证数据为空。\")\n\n    # 加载测试数据 (注意：测试集没有答案和文档 ID)\n    print(f\"\\n--- 加载测试数据 ---\")\n    test_data = load_jsonl(TEST_FILE)\n    test_df = list_to_dataframe(test_data)\n    if not test_df.empty:\n        print(f\"测试数据预览 (前 5 条):\")\n        print(test_df.head())\n        print(f\"\\n测试样本总数: {len(test_df)}\")\n        print(f\"列名: {test_df.columns.tolist()}\")\n        print(\"\\n测试数据缺失值检查:\")\n        print(test_df.isnull().sum()) # 预期 answer 和 document_id 列为 null\n    else:\n        print(\"未能加载测试数据或测试数据为空。\")\n\n    print(\"\\n数据加载和初步探索完成。\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T02:51:36.678532Z","iopub.execute_input":"2025-04-06T02:51:36.678907Z","iopub.status.idle":"2025-04-06T02:51:45.125462Z","shell.execute_reply.started":"2025-04-06T02:51:36.678876Z","shell.execute_reply":"2025-04-06T02:51:45.124627Z"}},"outputs":[{"name":"stdout","text":"开始加载 NQ10K 数据集...\n\n--- 加载文档 ---\n成功加载 12138 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n文档数据预览 (前 5 条):\n   document_id                                      document_text\n0            0  Email marketing - Wikipedia <H1> Email marketi...\n1            1  The Mother ( How I Met Your Mother ) - wikiped...\n2            2  Human fertilization - wikipedia <H1> Human fer...\n3            3  List of National Football League career quarte...\n4            4  Roanoke Colony - wikipedia <H1> Roanoke Colony...\n\n文档总数: 12138\n列名: ['document_id', 'document_text']\n\n文档数据缺失值检查:\ndocument_id      0\ndocument_text    0\ndtype: int64\n已将 12138 个文档存入字典。\n\n--- 加载训练数据 ---\n成功加载 8000 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/train.jsonl\n训练数据预览 (前 5 条):\n                                            question  \\\n0           what did the huns do to the roman empire   \n1       who won women's singles australian open 2018   \n2         who plays the gunslinger in the dark tower   \n3        what is the main language in czech republic   \n4  which mode of reasoning comes into play when s...   \n\n                          answer  document_id  \n0                       collapse         6205  \n1             Caroline Wozniacki         8985  \n2                     Idris Elba         9541  \n3                          Czech         2474  \n4  Cognitive-instrumental reason         6213  \n\n训练样本总数: 8000\n列名: ['question', 'answer', 'document_id']\n\n训练数据缺失值检查:\nquestion       0\nanswer         0\ndocument_id    0\ndtype: int64\n\n--- 加载验证数据 ---\n成功加载 1000 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n验证数据预览 (前 5 条):\n                                           question             answer  \\\n0  when did the british first land in north america               1607   \n1         when did the 1st world war officially end   11 November 1918   \n2    who's the girl that plays the new wonder woman  Gal Gadot-Varsano   \n3              who is the director of the cia today        Mike Pompeo   \n4           who plays ben in the new fantastic four         Jamie Bell   \n\n   document_id  \n0        11484  \n1        10722  \n2        11164  \n3        11721  \n4         9192  \n\n验证样本总数: 1000\n列名: ['question', 'answer', 'document_id']\n\n验证数据缺失值检查:\nquestion       0\nanswer         0\ndocument_id    0\ndtype: int64\n\n--- 加载测试数据 ---\n成功加载 1000 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/test.jsonl\n测试数据预览 (前 5 条):\n                                            question answer document_id\n0         what effect does the curvature of a mirror   None        None\n1  when did the movie jesus christ superstar come...   None        None\n2  one version of determinsm from eastern philoso...   None        None\n3           who plays hotch's wife in criminal minds   None        None\n4                who sings my woman my woman my wife   None        None\n\n测试样本总数: 1000\n列名: ['question', 'answer', 'document_id']\n\n测试数据缺失值检查:\nquestion          0\nanswer         1000\ndocument_id    1000\ndtype: int64\n\n数据加载和初步探索完成。\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\nimport os\nimport re # 用于正则表达式，处理 HTML 标签等\nimport pandas as pd\nimport nltk # 自然语言处理工具包\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# --- 配置 ---\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data' # 假设数据在 'data' 文件夹下\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl')\n\n# --- 下载 NLTK 数据 (如果尚未下载) ---\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'punkt' 数据...\")\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'stopwords' 数据...\")\n    nltk.download('stopwords', quiet=True)\n\n# --- 加载停用词 ---\n# 使用英文停用词列表\nstop_words = set(stopwords.words('english'))\n\n# --- 函数：加载 JSON Lines 文件 ---\ndef load_jsonl(file_path):\n    \"\"\"加载 JSON Lines 文件 (.jsonl) 并返回数据列表。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳过无法解析的行: {line.strip()} - 错误: {e}\")\n        print(f\"成功加载 {len(data)} 条记录从 {file_path}\")\n    except Exception as e:\n        print(f\"加载文件时出错 {file_path}: {e}\")\n    return data\n\n# --- 函数：文本预处理 ---\ndef preprocess_text(text):\n    \"\"\"\n    对文本进行预处理：\n    1. 移除 HTML 标签\n    2. 移除标点符号和数字\n    3. 转换为小写\n    4. 分词\n    5. 移除停用词\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    # 1. 移除 HTML 标签 (简单的正则表达式)\n    text = re.sub(r'<[^>]+>', ' ', text)\n    # 2. 移除标点符号和数字，只保留字母和空格\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # 3. 转换为小写\n    text = text.lower()\n    # 4. 分词\n    tokens = word_tokenize(text)\n    # 5. 移除停用词\n    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n    # 将处理后的词语列表重新组合成字符串，供 TfidfVectorizer 使用\n    return \" \".join(filtered_tokens)\n\n# --- TF-IDF 检索器类 ---\nclass TfidfRetriever:\n    \"\"\"\n    使用 TF-IDF 进行文档检索的类。\n    \"\"\"\n    def __init__(self):\n        # 初始化 TfidfVectorizer\n        # max_df=0.85: 忽略在超过 85% 的文档中出现的词语 (过于常见)\n        # min_df=2: 忽略在少于 2 个文档中出现的词语 (过于罕见或可能是噪音)\n        self.vectorizer = TfidfVectorizer(max_df=0.85, min_df=2)\n        self.tfidf_matrix = None\n        self.document_ids = []\n        self.documents_dict = {} # 存储原始文档内容，方便查看\n\n    def build_index(self, documents_data):\n        \"\"\"\n        使用文档数据构建 TF-IDF 索引。\n\n        Args:\n            documents_data (list[dict]): 从 documents.jsonl 加载的文档数据列表。\n                                         每个字典应包含 'document_id' 和 'document_text'。\n        \"\"\"\n        print(\"开始构建 TF-IDF 索引...\")\n        if not documents_data:\n            print(\"错误：文档数据为空，无法构建索引。\")\n            return\n\n        # 提取文档 ID 和文本\n        doc_texts = []\n        self.document_ids = []\n        self.documents_dict = {}\n\n        for doc in documents_data:\n            if 'document_id' in doc and 'document_text' in doc:\n                self.document_ids.append(doc['document_id'])\n                # 预处理文档文本\n                processed_text = preprocess_text(doc['document_text'])\n                doc_texts.append(processed_text)\n                self.documents_dict[doc['document_id']] = doc['document_text'] # 存储原始文本\n            else:\n                print(f\"警告: 跳过格式不正确的文档记录: {doc}\")\n\n        if not doc_texts:\n            print(\"错误：没有有效的文档文本用于构建索引。\")\n            return\n\n        # 拟合 TfidfVectorizer 并转换文档文本\n        print(f\"正在对 {len(doc_texts)} 个文档进行 TF-IDF 向量化...\")\n        self.tfidf_matrix = self.vectorizer.fit_transform(doc_texts)\n        print(f\"TF-IDF 索引构建完成。矩阵形状: {self.tfidf_matrix.shape}\")\n        print(f\"词汇表大小: {len(self.vectorizer.get_feature_names_out())}\")\n\n    def retrieve(self, query, top_n=5):\n        \"\"\"\n        根据查询检索最相关的 top_n 个文档 ID。\n\n        Args:\n            query (str): 用户输入的查询问题。\n            top_n (int): 要返回的最相关文档的数量。\n\n        Returns:\n            list[int]: 按相关性排序的 top_n 个文档 ID 列表。\n                       如果索引未构建或查询无效，则返回空列表。\n        \"\"\"\n        if self.tfidf_matrix is None or not self.document_ids:\n            print(\"错误：TF-IDF 索引尚未构建。请先调用 build_index()。\")\n            return []\n        if not query or not isinstance(query, str):\n            print(\"错误：查询无效。\")\n            return []\n\n        # 1. 预处理查询\n        processed_query = preprocess_text(query)\n        if not processed_query:\n            print(\"警告：预处理后的查询为空。\")\n            return []\n\n        # 2. 将查询转换为 TF-IDF 向量\n        # 注意：使用 transform 而不是 fit_transform，因为要用已有的词汇表和 IDF\n        query_vector = self.vectorizer.transform([processed_query])\n\n        # 3. 计算查询向量与所有文档向量的余弦相似度\n        # cosine_similarity 返回一个形状为 (n_queries, n_documents) 的矩阵\n        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n\n        # 4. 获取相似度最高的 top_n 个文档的索引\n        # argsort 返回排序后的索引，[::-1] 将其反转为降序\n        # [:top_n] 取前 n 个\n        top_n_indices = np.argsort(cosine_similarities)[::-1][:top_n]\n\n        # 5. 将索引映射回文档 ID\n        retrieved_doc_ids = [self.document_ids[i] for i in top_n_indices]\n\n        # (可选) 打印检索结果和相似度得分\n        # print(f\"\\n查询: '{query}'\")\n        # print(f\"检索到的 Top-{top_n} 文档 ID (及相似度):\")\n        # for i, doc_id in enumerate(retrieved_doc_ids):\n        #     similarity_score = cosine_similarities[top_n_indices[i]]\n        #     print(f\"  {i+1}. ID: {doc_id} (相似度: {similarity_score:.4f})\")\n            # print(f\"     内容预览: {self.documents_dict.get(doc_id, 'N/A')[:100]}...\") # 打印部分原文\n\n        return retrieved_doc_ids\n\n# --- 主程序：演示 TF-IDF 检索 ---\nif __name__ == \"__main__\":\n    # 1. 加载文档数据\n    print(\"--- 加载文档数据 ---\")\n    documents_data = load_jsonl(DOCUMENTS_FILE)\n\n    # 2. 初始化并构建 TF-IDF 检索器索引\n    retriever = TfidfRetriever()\n    retriever.build_index(documents_data)\n\n    # 3. 加载验证数据以获取示例问题\n    print(\"\\n--- 加载验证数据以获取示例 ---\")\n    val_data = load_jsonl(VAL_FILE)\n\n    if val_data and retriever.tfidf_matrix is not None:\n        # 4. 选择一个示例问题进行检索\n        example_index = 0 # 可以修改这个索引来测试不同的问题\n        example_question = val_data[example_index]['question']\n        true_doc_id = val_data[example_index]['document_id']\n\n        print(f\"\\n--- 使用 TF-IDF 进行检索示例 ---\")\n        print(f\"示例问题 (来自 val.jsonl[{example_index}]): '{example_question}'\")\n        print(f\"真实相关的文档 ID: {true_doc_id}\")\n\n        # 5. 执行检索\n        retrieved_ids = retriever.retrieve(example_question, top_n=5)\n\n        # 6. 显示检索结果\n        print(f\"\\nTF-IDF 检索到的 Top-5 文档 ID:\")\n        print(retrieved_ids)\n\n        # (可选) 检查真实文档是否在前 5 个结果中\n        if true_doc_id in retrieved_ids:\n            rank = retrieved_ids.index(true_doc_id) + 1\n            print(f\"成功! 真实文档 ID {true_doc_id} 在检索结果中排名第 {rank}。\")\n        else:\n            print(f\"失败。真实文档 ID {true_doc_id} 未在前 5 个检索结果中。\")\n\n        # (可选) 查看检索到的第一个文档的原文\n        if retrieved_ids:\n             first_retrieved_id = retrieved_ids[0]\n             print(f\"\\n检索到的第一个文档 (ID: {first_retrieved_id}) 的原文预览:\")\n             print(retriever.documents_dict.get(first_retrieved_id, \"未找到原文\")[:500] + \"...\") # 显示前 500 个字符\n\n    elif retriever.tfidf_matrix is None:\n         print(\"\\n由于索引构建失败，无法执行检索示例。\")\n    else:\n        print(\"\\n未能加载验证数据，无法执行检索示例。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:30:09.947009Z","iopub.execute_input":"2025-04-05T15:30:09.947294Z","iopub.status.idle":"2025-04-05T15:36:16.255840Z","shell.execute_reply.started":"2025-04-05T15:30:09.947270Z","shell.execute_reply":"2025-04-05T15:36:16.254855Z"}},"outputs":[{"name":"stdout","text":"--- 加载文档数据 ---\n成功加载 12138 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n开始构建 TF-IDF 索引...\n正在对 12138 个文档进行 TF-IDF 向量化...\nTF-IDF 索引构建完成。矩阵形状: (12138, 279270)\n词汇表大小: 279270\n\n--- 加载验证数据以获取示例 ---\n成功加载 1000 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n\n--- 使用 TF-IDF 进行检索示例 ---\n示例问题 (来自 val.jsonl[0]): 'when did the british first land in north america'\n真实相关的文档 ID: 11484\n\nTF-IDF 检索到的 Top-5 文档 ID:\n[8852, 7355, 11484, 1592, 860]\n成功! 真实文档 ID 11484 在检索结果中排名第 3。\n\n检索到的第一个文档 (ID: 8852) 的原文预览:\nGeography of North America - wikipedia <H1> Geography of North America </H1> Jump to : navigation , search Global view centered on North America <P> North America is the third largest continent , and is also a portion of the second largest supercontinent if North and South America are combined into the Americas and Africa , Europe , and Asia are considered to be part of one supercontinent called Afro - Eurasia . </P> <P> With an estimated population of 460 million and an area of 24,346,000 km2 (...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install rank_bm25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:36:16.257351Z","iopub.execute_input":"2025-04-05T15:36:16.257866Z","iopub.status.idle":"2025-04-05T15:36:22.042518Z","shell.execute_reply.started":"2025-04-05T15:36:16.257833Z","shell.execute_reply":"2025-04-05T15:36:22.041417Z"}},"outputs":[{"name":"stdout","text":"Collecting rank_bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rank_bm25) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rank_bm25) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rank_bm25) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rank_bm25) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rank_bm25) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rank_bm25) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rank_bm25) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rank_bm25) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rank_bm25) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rank_bm25) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rank_bm25) (2024.2.0)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nInstalling collected packages: rank_bm25\nSuccessfully installed rank_bm25-0.2.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import json\nimport os\nimport re # 用于正则表达式\nimport pandas as pd\nimport nltk # 自然语言处理工具包\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom rank_bm25 import BM25Okapi # BM25 库\nimport numpy as np\nimport time # 用于计时\n\n# --- 配置 ---\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data' # 假设数据在 'data' 文件夹下\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl')\n\n# --- 下载 NLTK 数据 (如果尚未下载) ---\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'punkt' 数据...\")\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'stopwords' 数据...\")\n    nltk.download('stopwords', quiet=True)\n\n# --- 加载停用词 ---\nstop_words = set(stopwords.words('english'))\n\n# --- 函数：加载 JSON Lines 文件 ---\ndef load_jsonl(file_path):\n    \"\"\"加载 JSON Lines 文件 (.jsonl) 并返回数据列表。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳过无法解析的行: {line.strip()} - 错误: {e}\")\n        print(f\"成功加载 {len(data)} 条记录从 {file_path}\")\n    except Exception as e:\n        print(f\"加载文件时出错 {file_path}: {e}\")\n    return data\n\n# --- 函数：文本预处理 (适用于 BM25，返回 token 列表) ---\ndef preprocess_text_bm25(text):\n    \"\"\"\n    对文本进行预处理，返回 token 列表：\n    1. 移除 HTML 标签\n    2. 移除标点符号和数字\n    3. 转换为小写\n    4. 分词\n    5. 移除停用词\n    \"\"\"\n    if not isinstance(text, str):\n        return []\n    # 1. 移除 HTML 标签\n    text = re.sub(r'<[^>]+>', ' ', text)\n    # 2. 移除标点符号和数字\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # 3. 转换为小写\n    text = text.lower()\n    # 4. 分词\n    tokens = word_tokenize(text)\n    # 5. 移除停用词，并确保是字母\n    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n    return filtered_tokens\n\n# --- BM25 检索器类 ---\nclass Bm25Retriever:\n    \"\"\"\n    使用 BM25 进行文档检索的类。\n    \"\"\"\n    def __init__(self):\n        self.bm25_index = None\n        self.document_ids = []\n        self.documents_dict = {} # 存储原始文档内容\n\n    def build_index(self, documents_data):\n        \"\"\"\n        使用文档数据构建 BM25 索引。\n\n        Args:\n            documents_data (list[dict]): 从 documents.jsonl 加载的文档数据列表。\n        \"\"\"\n        print(\"开始构建 BM25 索引...\")\n        start_time = time.time()\n        if not documents_data:\n            print(\"错误：文档数据为空，无法构建索引。\")\n            return\n\n        tokenized_corpus = []\n        self.document_ids = []\n        self.documents_dict = {}\n\n        print(f\"正在对 {len(documents_data)} 个文档进行预处理和分词...\")\n        count = 0\n        for doc in documents_data:\n            if 'document_id' in doc and 'document_text' in doc:\n                self.document_ids.append(doc['document_id'])\n                self.documents_dict[doc['document_id']] = doc['document_text']\n                # 使用适用于 BM25 的预处理函数\n                processed_tokens = preprocess_text_bm25(doc['document_text'])\n                tokenized_corpus.append(processed_tokens)\n                count += 1\n                if count % 1000 == 0:\n                    print(f\"  已处理 {count}/{len(documents_data)} 个文档...\")\n            else:\n                print(f\"警告: 跳过格式不正确的文档记录: {doc}\")\n\n        if not tokenized_corpus:\n            print(\"错误：没有有效的文档文本用于构建索引。\")\n            return\n\n        print(f\"\\n使用 {len(tokenized_corpus)} 个文档的 token 列表初始化 BM25Okapi...\")\n        # 使用分词后的文档列表初始化 BM25Okapi\n        self.bm25_index = BM25Okapi(tokenized_corpus)\n        end_time = time.time()\n        print(f\"BM25 索引构建完成。耗时: {end_time - start_time:.2f} 秒\")\n\n    def retrieve(self, query, top_n=5):\n        \"\"\"\n        根据查询检索最相关的 top_n 个文档 ID。\n\n        Args:\n            query (str): 用户输入的查询问题。\n            top_n (int): 要返回的最相关文档的数量。\n\n        Returns:\n            list[int]: 按相关性排序的 top_n 个文档 ID 列表。\n        \"\"\"\n        if self.bm25_index is None or not self.document_ids:\n            print(\"错误：BM25 索引尚未构建。请先调用 build_index()。\")\n            return []\n        if not query or not isinstance(query, str):\n            print(\"错误：查询无效。\")\n            return []\n\n        # 1. 预处理查询 (得到 token 列表)\n        tokenized_query = preprocess_text_bm25(query)\n        if not tokenized_query:\n            print(\"警告：预处理后的查询为空。\")\n            return []\n\n        # 2. 计算查询与所有文档的 BM25 得分\n        # get_scores 返回一个包含所有文档得分的 numpy 数组\n        doc_scores = self.bm25_index.get_scores(tokenized_query)\n\n        # 3. 获取得分最高的 top_n 个文档的索引\n        # argsort 返回排序后的索引，[::-1] 将其反转为降序\n        top_n_indices = np.argsort(doc_scores)[::-1][:top_n]\n\n        # 4. 将索引映射回文档 ID\n        retrieved_doc_ids = [self.document_ids[i] for i in top_n_indices]\n\n        # (可选) 打印检索结果和得分\n        # print(f\"\\n查询: '{query}'\")\n        # print(f\"BM25 检索到的 Top-{top_n} 文档 ID (及得分):\")\n        # for i, doc_id in enumerate(retrieved_doc_ids):\n        #     score = doc_scores[top_n_indices[i]]\n        #     print(f\"  {i+1}. ID: {doc_id} (得分: {score:.4f})\")\n\n        return retrieved_doc_ids\n\n# --- 主程序：演示 BM25 检索 ---\nif __name__ == \"__main__\":\n    # 1. 加载文档数据\n    print(\"--- 加载文档数据 ---\")\n    documents_data = load_jsonl(DOCUMENTS_FILE)\n\n    # 2. 初始化并构建 BM25 检索器索引\n    retriever = Bm25Retriever()\n    retriever.build_index(documents_data)\n\n    # 3. 加载验证数据以获取示例问题\n    print(\"\\n--- 加载验证数据以获取示例 ---\")\n    val_data = load_jsonl(VAL_FILE)\n\n    if val_data and retriever.bm25_index is not None:\n        # 4. 选择与 TF-IDF 相同的示例问题进行检索\n        example_index = 0 # 与 TF-IDF 示例保持一致\n        example_question = val_data[example_index]['question']\n        true_doc_id = val_data[example_index]['document_id']\n\n        print(f\"\\n--- 使用 BM25 进行检索示例 ---\")\n        print(f\"示例问题 (来自 val.jsonl[{example_index}]): '{example_question}'\")\n        print(f\"真实相关的文档 ID: {true_doc_id}\")\n\n        # 5. 执行检索\n        retrieved_ids = retriever.retrieve(example_question, top_n=5)\n\n        # 6. 显示检索结果\n        print(f\"\\nBM25 检索到的 Top-5 文档 ID:\")\n        print(retrieved_ids)\n\n        # (可选) 检查真实文档是否在前 5 个结果中\n        if true_doc_id in retrieved_ids:\n            rank = retrieved_ids.index(true_doc_id) + 1\n            print(f\"成功! 真实文档 ID {true_doc_id} 在检索结果中排名第 {rank}。\")\n        else:\n            print(f\"失败。真实文档 ID {true_doc_id} 未在前 5 个检索结果中。\")\n\n        # (可选) 查看检索到的第一个文档的原文\n        if retrieved_ids:\n             first_retrieved_id = retrieved_ids[0]\n             print(f\"\\n检索到的第一个文档 (ID: {first_retrieved_id}) 的原文预览:\")\n             print(retriever.documents_dict.get(first_retrieved_id, \"未找到原文\")[:500] + \"...\")\n\n    elif retriever.bm25_index is None:\n         print(\"\\n由于索引构建失败，无法执行检索示例。\")\n    else:\n        print(\"\\n未能加载验证数据，无法执行检索示例。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:36:22.043960Z","iopub.execute_input":"2025-04-05T15:36:22.044301Z","iopub.status.idle":"2025-04-05T15:42:10.352953Z","shell.execute_reply.started":"2025-04-05T15:36:22.044274Z","shell.execute_reply":"2025-04-05T15:42:10.352068Z"}},"outputs":[{"name":"stdout","text":"--- 加载文档数据 ---\n成功加载 12138 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n开始构建 BM25 索引...\n正在对 12138 个文档进行预处理和分词...\n  已处理 1000/12138 个文档...\n  已处理 2000/12138 个文档...\n  已处理 3000/12138 个文档...\n  已处理 4000/12138 个文档...\n  已处理 5000/12138 个文档...\n  已处理 6000/12138 个文档...\n  已处理 7000/12138 个文档...\n  已处理 8000/12138 个文档...\n  已处理 9000/12138 个文档...\n  已处理 10000/12138 个文档...\n  已处理 11000/12138 个文档...\n  已处理 12000/12138 个文档...\n\n使用 12138 个文档的 token 列表初始化 BM25Okapi...\nBM25 索引构建完成。耗时: 342.66 秒\n\n--- 加载验证数据以获取示例 ---\n成功加载 1000 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n\n--- 使用 BM25 进行检索示例 ---\n示例问题 (来自 val.jsonl[0]): 'when did the british first land in north america'\n真实相关的文档 ID: 11484\n\nBM25 检索到的 Top-5 文档 ID:\n[11423, 7092, 411, 2358, 3066]\n失败。真实文档 ID 11484 未在前 5 个检索结果中。\n\n检索到的第一个文档 (ID: 11423) 的原文预览:\nHistory of Antarctica - Wikipedia <H1> History of Antarctica </H1> Jump to : navigation , search <Dl> <Dd> For the natural history of the Antarctic continent , see Antarctica . </Dd> </Dl> Painting of James Weddell 's second expedition , depicting the brig Jane and the cutter Beaufroy . <P> The history of Antarctica emerges from early Western theories of a vast continent , known as Terra Australis , believed to exist in the far south of the globe . The term Antarctic , referring to the opposite ...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import json\nimport os\nimport re\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport gensim # 用于加载和使用 FastText/Word2Vec 模型\n# 不再需要 downloader，因为我们直接加载本地文件\n# import gensim.downloader as api\nfrom gensim.models import KeyedVectors # 用于加载 .vec 文件\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport time\n\n# --- 配置 ---\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data' # 假设数据在 'data' 文件夹下\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl')\n\n# --- 重要：修改这里 ---\n# 将 PRETRAINED_MODEL_PATH 设置为你的本地 .vec 文件路径\nPRETRAINED_MODEL_PATH = '/kaggle/input/crawl-300d-2m/crawl-300d-2M.vec'\n# 如果你想尝试 gensim downloader (需要网络和下载)，可以取消注释下面这行\n# PRETRAINED_MODEL_NAME_DOWNLOAD = 'fasttext-wiki-news-subwords-300'\n\n# --- 下载 NLTK 数据 (如果尚未下载) ---\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'punkt' 数据...\")\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'stopwords' 数据...\")\n    nltk.download('stopwords', quiet=True)\n\n# --- 加载停用词 ---\nstop_words = set(stopwords.words('english'))\n\n# --- 函数：加载 JSON Lines 文件 ---\ndef load_jsonl(file_path):\n    \"\"\"加载 JSON Lines 文件 (.jsonl) 并返回数据列表。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳过无法解析的行: {line.strip()} - 错误: {e}\")\n        print(f\"成功加载 {len(data)} 条记录从 {file_path}\")\n    except Exception as e:\n        print(f\"加载文件时出错 {file_path}: {e}\")\n    return data\n\n# --- 函数：文本预处理 (返回 token 列表) ---\ndef preprocess_text_tokens(text):\n    \"\"\"\n    对文本进行预处理，返回 token 列表：\n    1. 移除 HTML 标签\n    2. 移除标点符号和数字\n    3. 转换为小写\n    4. 分词\n    5. 移除停用词\n    \"\"\"\n    if not isinstance(text, str):\n        return []\n    # 1. 移除 HTML 标签\n    text = re.sub(r'<[^>]+>', ' ', text)\n    # 2. 移除标点符号和数字\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # 3. 转换为小写\n    text = text.lower()\n    # 4. 分词\n    tokens = word_tokenize(text)\n    # 5. 移除停用词\n    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n    return filtered_tokens\n\n# --- 函数：计算文本的平均词向量 ---\ndef get_average_vector(tokens, model):\n    \"\"\"\n    计算给定 token 列表的平均词向量。\n    忽略模型词汇表中不存在的词。\n    \"\"\"\n    vectors = []\n    # KeyedVectors 使用 model.key_to_index 检查词是否存在\n    # 或者直接尝试访问，如果不存在会抛出 KeyError\n    for token in tokens:\n        try:\n            vectors.append(model[token])\n        except KeyError:\n            # 忽略不在词汇表中的词\n            pass\n\n    if not vectors:\n        # 如果文本中没有词在模型词汇表中，返回零向量\n        return np.zeros(model.vector_size)\n\n    # 计算向量的平均值\n    return np.mean(vectors, axis=0)\n\n# --- 词向量检索器类 (名称保持不变，但现在加载 .vec) ---\nclass FastTextRetriever:\n    \"\"\"\n    使用预训练词向量 (.vec 文件) 进行文档检索的类。\n    \"\"\"\n    def __init__(self, model_path=PRETRAINED_MODEL_PATH):\n        self.model_path = model_path # 修改为 model_path\n        self.model = None\n        self.doc_vectors = None\n        self.document_ids = []\n        self.documents_dict = {} # 存储原始文档内容\n\n    def load_model(self):\n        \"\"\"从本地文件路径加载预训练的词向量模型 (.vec 格式)。\"\"\"\n        print(f\"开始从本地文件加载预训练模型: {self.model_path} ...\")\n        print(\"这可能需要一些时间，取决于模型大小和你的机器性能。\")\n        start_time = time.time()\n        if not os.path.exists(self.model_path):\n             print(f\"错误: 模型文件未找到: {self.model_path}\")\n             self.model = None\n             return\n        try:\n            # --- 修改：使用 KeyedVectors.load_word2vec_format 加载 ---\n            # binary=False 表示是文本格式的 .vec 文件\n            # limit 参数可以限制加载的向量数量，用于快速测试 (例如 limit=500000)\n            # 如果内存足够，可以移除 limit 参数加载完整模型\n            self.model = KeyedVectors.load_word2vec_format(self.model_path, binary=False) #, limit=500000)\n            end_time = time.time()\n            print(f\"模型加载完成。耗时: {end_time - start_time:.2f} 秒\")\n            print(f\"向量维度: {self.model.vector_size}\")\n            print(f\"词汇表大小: {len(self.model.key_to_index)}\") # 显示加载的词汇量\n        except Exception as e:\n            print(f\"加载模型时发生错误: {e}\")\n            print(\"请确保文件路径正确且文件是有效的 .vec 格式。\")\n            self.model = None\n\n    def build_index(self, documents_data):\n        \"\"\"\n        使用文档数据计算并存储文档向量。\n\n        Args:\n            documents_data (list[dict]): 从 documents.jsonl 加载的文档数据列表。\n        \"\"\"\n        if self.model is None:\n            print(\"错误：预训练模型未加载。请先调用 load_model()。\")\n            return\n\n        print(\"开始构建文档向量索引...\")\n        start_time = time.time()\n        if not documents_data:\n            print(\"错误：文档数据为空，无法构建索引。\")\n            return\n\n        doc_vectors_list = []\n        self.document_ids = []\n        self.documents_dict = {}\n\n        print(f\"正在对 {len(documents_data)} 个文档进行预处理和向量化...\")\n        count = 0\n        for doc in documents_data:\n            if 'document_id' in doc and 'document_text' in doc:\n                self.document_ids.append(doc['document_id'])\n                self.documents_dict[doc['document_id']] = doc['document_text']\n                # 预处理文档文本得到 tokens\n                processed_tokens = preprocess_text_tokens(doc['document_text'])\n                # 计算文档的平均向量\n                doc_vector = get_average_vector(processed_tokens, self.model)\n                doc_vectors_list.append(doc_vector)\n                count += 1\n                if count % 1000 == 0:\n                    print(f\"  已处理 {count}/{len(documents_data)} 个文档...\")\n            else:\n                print(f\"警告: 跳过格式不正确的文档记录: {doc}\")\n\n        if not doc_vectors_list:\n            print(\"错误：未能为任何文档生成向量。\")\n            return\n\n        # 将向量列表转换为 numpy 数组，方便计算\n        self.doc_vectors = np.array(doc_vectors_list)\n        end_time = time.time()\n        print(f\"文档向量索引构建完成。向量矩阵形状: {self.doc_vectors.shape}\")\n        print(f\"耗时: {end_time - start_time:.2f} 秒\")\n\n\n    def retrieve(self, query, top_n=5):\n        \"\"\"\n        根据查询检索最相关的 top_n 个文档 ID。\n\n        Args:\n            query (str): 用户输入的查询问题。\n            top_n (int): 要返回的最相关文档的数量。\n\n        Returns:\n            list[int]: 按相关性排序的 top_n 个文档 ID 列表。\n        \"\"\"\n        if self.model is None or self.doc_vectors is None or not self.document_ids:\n            print(\"错误：模型或文档向量索引尚未构建。\")\n            return []\n        if not query or not isinstance(query, str):\n            print(\"错误：查询无效。\")\n            return []\n\n        # 1. 预处理查询并计算查询向量\n        tokenized_query = preprocess_text_tokens(query)\n        if not tokenized_query:\n            print(\"警告：预处理后的查询为空。\")\n            return []\n        query_vector = get_average_vector(tokenized_query, self.model)\n\n        # 如果查询向量全为零 (例如，查询中所有词都不在模型词汇表里)\n        if np.all(query_vector == 0):\n             print(\"警告：无法为查询生成有效向量 (可能所有词都不在词汇表中)。\")\n             return []\n\n        # 2. 计算查询向量与所有文档向量的余弦相似度\n        # query_vector 需要 reshape 成 (1, vector_size) 来进行计算\n        cosine_similarities = cosine_similarity(query_vector.reshape(1, -1), self.doc_vectors).flatten()\n\n        # 3. 获取相似度最高的 top_n 个文档的索引\n        top_n_indices = np.argsort(cosine_similarities)[::-1][:top_n]\n\n        # 4. 将索引映射回文档 ID\n        retrieved_doc_ids = [self.document_ids[i] for i in top_n_indices]\n\n        # (可选) 打印检索结果和得分\n        # print(f\"\\n查询: '{query}'\")\n        # print(f\"词向量检索到的 Top-{top_n} 文档 ID (及相似度):\")\n        # for i, doc_id in enumerate(retrieved_doc_ids):\n        #     similarity_score = cosine_similarities[top_n_indices[i]]\n        #     print(f\"  {i+1}. ID: {doc_id} (相似度: {similarity_score:.4f})\")\n\n        return retrieved_doc_ids\n\n# --- 主程序：演示词向量检索 ---\nif __name__ == \"__main__\":\n    # 1. 初始化检索器并加载预训练模型 (从本地文件)\n    print(\"--- 初始化词向量检索器并加载本地模型 ---\")\n    # 注意：这里的 model_path 参数现在指向你的本地文件\n    retriever = FastTextRetriever(model_path=PRETRAINED_MODEL_PATH)\n    retriever.load_model() # 加载本地模型\n\n    if retriever.model: # 仅在模型加载成功后继续\n        # 2. 加载文档数据\n        print(\"\\n--- 加载文档数据 ---\")\n        documents_data = load_jsonl(DOCUMENTS_FILE)\n\n        # 3. 构建文档向量索引\n        retriever.build_index(documents_data)\n\n        # 4. 加载验证数据以获取示例问题\n        print(\"\\n--- 加载验证数据以获取示例 ---\")\n        val_data = load_jsonl(VAL_FILE)\n\n        if val_data and retriever.doc_vectors is not None:\n            # 5. 选择与之前相同的示例问题进行检索\n            example_index = 0\n            example_question = val_data[example_index]['question']\n            true_doc_id = val_data[example_index]['document_id']\n\n            print(f\"\\n--- 使用词向量进行检索示例 ---\")\n            print(f\"示例问题 (来自 val.jsonl[{example_index}]): '{example_question}'\")\n            print(f\"真实相关的文档 ID: {true_doc_id}\")\n\n            # 6. 执行检索\n            retrieved_ids = retriever.retrieve(example_question, top_n=5)\n\n            # 7. 显示检索结果\n            print(f\"\\n词向量检索到的 Top-5 文档 ID:\")\n            print(retrieved_ids)\n\n            # (可选) 检查真实文档是否在前 5 个结果中\n            if retrieved_ids and true_doc_id in retrieved_ids:\n                rank = retrieved_ids.index(true_doc_id) + 1\n                print(f\"成功! 真实文档 ID {true_doc_id} 在检索结果中排名第 {rank}。\")\n            elif retrieved_ids:\n                print(f\"失败。真实文档 ID {true_doc_id} 未在前 5 个检索结果中。\")\n            else:\n                print(\"未能检索到任何文档。\")\n\n\n            # (可选) 查看检索到的第一个文档的原文\n            if retrieved_ids:\n                 first_retrieved_id = retrieved_ids[0]\n                 print(f\"\\n检索到的第一个文档 (ID: {first_retrieved_id}) 的原文预览:\")\n                 print(retriever.documents_dict.get(first_retrieved_id, \"未找到原文\")[:500] + \"...\")\n\n        elif retriever.doc_vectors is None:\n             print(\"\\n由于文档向量索引构建失败，无法执行检索示例。\")\n        else:\n            print(\"\\n未能加载验证数据，无法执行检索示例。\")\n    else:\n        print(\"\\n由于模型加载失败，无法继续执行。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:42:10.354203Z","iopub.execute_input":"2025-04-05T15:42:10.354473Z","iopub.status.idle":"2025-04-05T15:57:04.236850Z","shell.execute_reply.started":"2025-04-05T15:42:10.354451Z","shell.execute_reply":"2025-04-05T15:57:04.235644Z"}},"outputs":[{"name":"stdout","text":"--- 初始化词向量检索器并加载本地模型 ---\n开始从本地文件加载预训练模型: /kaggle/input/crawl-300d-2m/crawl-300d-2M.vec ...\n这可能需要一些时间，取决于模型大小和你的机器性能。\n模型加载完成。耗时: 416.74 秒\n向量维度: 300\n词汇表大小: 1999995\n\n--- 加载文档数据 ---\n成功加载 12138 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n开始构建文档向量索引...\n正在对 12138 个文档进行预处理和向量化...\n  已处理 1000/12138 个文档...\n  已处理 2000/12138 个文档...\n  已处理 3000/12138 个文档...\n  已处理 4000/12138 个文档...\n  已处理 5000/12138 个文档...\n  已处理 6000/12138 个文档...\n  已处理 7000/12138 个文档...\n  已处理 8000/12138 个文档...\n  已处理 9000/12138 个文档...\n  已处理 10000/12138 个文档...\n  已处理 11000/12138 个文档...\n  已处理 12000/12138 个文档...\n文档向量索引构建完成。向量矩阵形状: (12138, 300)\n耗时: 453.60 秒\n\n--- 加载验证数据以获取示例 ---\n成功加载 1000 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n\n--- 使用词向量进行检索示例 ---\n示例问题 (来自 val.jsonl[0]): 'when did the british first land in north america'\n真实相关的文档 ID: 11484\n\n词向量检索到的 Top-5 文档 ID:\n[11484, 81, 4117, 5493, 570]\n成功! 真实文档 ID 11484 在检索结果中排名第 1。\n\n检索到的第一个文档 (ID: 11484) 的原文预览:\nBritish colonization of the Americas - wikipedia <H1> British colonization of the Americas </H1> <P> </P> <Table> <Tr> <Td> Part of a series on </Td> </Tr> <Tr> <Th> European colonization of the Americas </Th> </Tr> <Tr> <Td> </Td> </Tr> <Tr> <Td> <Ul> <Li> First wave of European colonization </Li> <Li> British </Li> <Li> Couronian </Li> <Li> Danish </Li> <Li> Dutch </Li> <Li> French </Li> <Li> German </Li> <Li> Hospitaller ( Maltese ) </Li> <Li> Norse </Li> <Li> Portuguese </Li> <Li> Russian </...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import json\nimport os\nimport re\nimport time\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom rank_bm25 import BM25Okapi\nfrom gensim.models import KeyedVectors\nimport numpy as np\nfrom tqdm import tqdm # 用于显示进度条\n\n# --- 配置 ---\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data'\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl')\n# 你的本地 .vec 文件路径\nPRETRAINED_MODEL_PATH = '/kaggle/input/crawl-300d-2m/crawl-300d-2M.vec'\n\n# --- NLTK 数据下载检查 ---\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'punkt' 数据...\")\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept nltk.downloader.DownloadError:\n    print(\"下载 NLTK 'stopwords' 数据...\")\n    nltk.download('stopwords', quiet=True)\n\nstop_words = set(stopwords.words('english'))\n\n# --- 辅助函数 ---\n\ndef load_jsonl(file_path):\n    \"\"\"加载 JSON Lines 文件。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            # 使用 tqdm 显示文件读取进度 (如果文件很大)\n            total_lines = sum(1 for line in f) # 计算总行数可能较慢\n            f.seek(0) # 重置文件指针\n            for line in tqdm(f, total=total_lines, desc=f\"加载 {os.path.basename(file_path)}\"):\n            # for line in f: # 不显示进度条的简化版本\n                 if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳过无法解析的行: {line.strip()} - 错误: {e}\")\n        print(f\"成功加载 {len(data)} 条记录从 {file_path}\")\n    except Exception as e:\n        print(f\"加载文件时出错 {file_path}: {e}\")\n    return data\n\ndef preprocess_text_tfidf(text):\n    \"\"\"文本预处理 (TF-IDF): 返回处理后的字符串。\"\"\"\n    if not isinstance(text, str): return \"\"\n    text = re.sub(r'<[^>]+>', ' ', text)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = text.lower()\n    tokens = word_tokenize(text)\n    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n    return \" \".join(filtered_tokens)\n\ndef preprocess_text_tokens(text):\n    \"\"\"文本预处理 (BM25/FastText): 返回 token 列表。\"\"\"\n    if not isinstance(text, str): return []\n    text = re.sub(r'<[^>]+>', ' ', text)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = text.lower()\n    tokens = word_tokenize(text)\n    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n    return filtered_tokens\n\ndef get_average_vector(tokens, model):\n    \"\"\"计算平均词向量。\"\"\"\n    vectors = []\n    for token in tokens:\n        try:\n            vectors.append(model[token])\n        except KeyError:\n            pass\n    if not vectors: return np.zeros(model.vector_size)\n    return np.mean(vectors, axis=0)\n\n# --- 检索器类定义 (从之前步骤复制并整合) ---\n\nclass BaseRetriever:\n    \"\"\"检索器基类，定义通用接口。\"\"\"\n    def __init__(self):\n        self.document_ids = []\n        self.documents_dict = {}\n\n    def build_index(self, documents_data):\n        raise NotImplementedError\n\n    def retrieve(self, query, top_n=5):\n        raise NotImplementedError\n\n    def _prepare_docs(self, documents_data):\n        \"\"\"提取文档 ID 和内容字典。\"\"\"\n        self.document_ids = [doc['document_id'] for doc in documents_data if 'document_id' in doc]\n        self.documents_dict = {doc['document_id']: doc['document_text'] for doc in documents_data if 'document_id' in doc and 'document_text' in doc}\n        print(f\"准备了 {len(self.document_ids)} 个文档 ID 和 {len(self.documents_dict)} 个文档原文。\")\n        return documents_data # 返回原始数据供子类使用\n\nclass TfidfRetriever(BaseRetriever):\n    \"\"\"TF-IDF 检索器。\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.vectorizer = TfidfVectorizer(max_df=0.85, min_df=2)\n        self.tfidf_matrix = None\n\n    def build_index(self, documents_data):\n        print(\"\\n--- 构建 TF-IDF 索引 ---\")\n        start_time = time.time()\n        documents_data = self._prepare_docs(documents_data)\n        doc_texts = [preprocess_text_tfidf(self.documents_dict.get(doc_id, '')) for doc_id in self.document_ids]\n        if not doc_texts: print(\"错误：无有效文档文本。\"); return\n        self.tfidf_matrix = self.vectorizer.fit_transform(doc_texts)\n        print(f\"TF-IDF 索引构建完成。矩阵形状: {self.tfidf_matrix.shape}. 耗时: {time.time() - start_time:.2f} 秒\")\n\n    def retrieve(self, query, top_n=5):\n        if self.tfidf_matrix is None: return []\n        processed_query = preprocess_text_tfidf(query)\n        if not processed_query: return []\n        query_vector = self.vectorizer.transform([processed_query])\n        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n        # 使用 argpartition 获取 top_n 最快，然后排序这 top_n 个\n        # top_n_indices = np.argsort(cosine_similarities)[::-1][:top_n]\n        k = min(top_n, self.tfidf_matrix.shape[0]) # 确保 top_n 不超过文档数\n        top_n_indices = np.argpartition(cosine_similarities, -k)[-k:]\n        # 对这 k 个索引按得分排序\n        top_n_indices = top_n_indices[np.argsort(cosine_similarities[top_n_indices])[::-1]]\n        return [self.document_ids[i] for i in top_n_indices]\n\nclass Bm25Retriever(BaseRetriever):\n    \"\"\"BM25 检索器。\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.bm25_index = None\n        self.tokenized_corpus = []\n\n    def build_index(self, documents_data):\n        print(\"\\n--- 构建 BM25 索引 ---\")\n        start_time = time.time()\n        documents_data = self._prepare_docs(documents_data)\n        print(f\"正在对 {len(self.document_ids)} 个文档进行预处理和分词...\")\n        self.tokenized_corpus = [preprocess_text_tokens(self.documents_dict.get(doc_id, '')) for doc_id in tqdm(self.document_ids, desc=\"BM25 Preprocessing\")]\n        if not self.tokenized_corpus: print(\"错误：无有效文档文本。\"); return\n        print(f\"\\n初始化 BM25Okapi...\")\n        self.bm25_index = BM25Okapi(self.tokenized_corpus)\n        print(f\"BM25 索引构建完成。耗时: {time.time() - start_time:.2f} 秒\")\n\n    def retrieve(self, query, top_n=5):\n        if self.bm25_index is None: return []\n        tokenized_query = preprocess_text_tokens(query)\n        if not tokenized_query: return []\n        doc_scores = self.bm25_index.get_scores(tokenized_query)\n        # top_n_indices = np.argsort(doc_scores)[::-1][:top_n]\n        k = min(top_n, len(self.document_ids))\n        top_n_indices = np.argpartition(doc_scores, -k)[-k:]\n        top_n_indices = top_n_indices[np.argsort(doc_scores[top_n_indices])[::-1]]\n        return [self.document_ids[i] for i in top_n_indices]\n\nclass FastTextRetriever(BaseRetriever):\n    \"\"\"词向量检索器。\"\"\"\n    def __init__(self, model_path=PRETRAINED_MODEL_PATH):\n        super().__init__()\n        self.model_path = model_path\n        self.model = None\n        self.doc_vectors = None\n\n    def load_model(self):\n        print(f\"\\n--- 加载词向量模型 ({os.path.basename(self.model_path)}) ---\")\n        start_time = time.time()\n        if not os.path.exists(self.model_path): print(f\"错误: 模型文件未找到: {self.model_path}\"); return\n        try:\n            self.model = KeyedVectors.load_word2vec_format(self.model_path, binary=False)\n            print(f\"模型加载完成。耗时: {time.time() - start_time:.2f} 秒. V={len(self.model.key_to_index)}, D={self.model.vector_size}\")\n        except Exception as e: print(f\"加载模型时发生错误: {e}\"); self.model = None\n\n    def build_index(self, documents_data):\n        if self.model is None: print(\"错误：模型未加载。\"); return\n        print(\"\\n--- 构建文档向量索引 ---\")\n        start_time = time.time()\n        documents_data = self._prepare_docs(documents_data)\n        print(f\"正在对 {len(self.document_ids)} 个文档进行预处理和向量化...\")\n        doc_vectors_list = [get_average_vector(preprocess_text_tokens(self.documents_dict.get(doc_id, '')), self.model)\n                            for doc_id in tqdm(self.document_ids, desc=\"Vectorizing Docs\")]\n        if not doc_vectors_list: print(\"错误：未能生成向量。\"); return\n        self.doc_vectors = np.array(doc_vectors_list)\n        print(f\"文档向量索引构建完成。矩阵形状: {self.doc_vectors.shape}. 耗时: {time.time() - start_time:.2f} 秒\")\n\n    def retrieve(self, query, top_n=5):\n        if self.model is None or self.doc_vectors is None: return []\n        tokenized_query = preprocess_text_tokens(query)\n        if not tokenized_query: return []\n        query_vector = get_average_vector(tokenized_query, self.model)\n        if np.all(query_vector == 0): return []\n        cosine_similarities = cosine_similarity(query_vector.reshape(1, -1), self.doc_vectors).flatten()\n        # top_n_indices = np.argsort(cosine_similarities)[::-1][:top_n]\n        k = min(top_n, self.doc_vectors.shape[0])\n        top_n_indices = np.argpartition(cosine_similarities, -k)[-k:]\n        top_n_indices = top_n_indices[np.argsort(cosine_similarities[top_n_indices])[::-1]]\n        return [self.document_ids[i] for i in top_n_indices]\n\n# --- 评估函数 ---\ndef evaluate_retriever(retriever, validation_data, top_n=5):\n    \"\"\"\n    评估给定检索器在验证集上的 Recall@N 和 MRR@N。\n\n    Args:\n        retriever (BaseRetriever): 已构建好索引的检索器对象。\n        validation_data (list[dict]): 验证数据集。\n        top_n (int): 评估指标的 N 值 (例如 Recall@5, MRR@5)。\n\n    Returns:\n        dict: 包含 'recall@N' 和 'mrr@N' 的字典。\n    \"\"\"\n    recall_sum = 0\n    mrr_sum = 0\n    total = len(validation_data)\n\n    if total == 0:\n        return {f'recall@{top_n}': 0, f'mrr@{top_n}': 0}\n\n    print(f\"\\n开始评估 {type(retriever).__name__} (共 {total} 个问题)...\")\n    # 使用 tqdm 显示评估进度\n    for item in tqdm(validation_data, desc=f\"Evaluating {type(retriever).__name__}\"):\n        question = item['question']\n        true_doc_id = item['document_id']\n\n        # 执行检索\n        retrieved_ids = retriever.retrieve(question, top_n=top_n)\n\n        # 计算 Recall@N\n        if true_doc_id in retrieved_ids:\n            recall_sum += 1\n\n            # 计算 MRR@N\n            try:\n                rank = retrieved_ids.index(true_doc_id) + 1\n                mrr_sum += 1.0 / rank\n            except ValueError:\n                #理论上如果 recall_sum+=1，这里不会发生，但为了健壮性保留\n                pass # MRR 贡献为 0\n\n    recall_at_n = recall_sum / total\n    mrr_at_n = mrr_sum / total\n\n    return {f'recall@{top_n}': recall_at_n, f'mrr@{top_n}': mrr_at_n}\n\n# --- 主评估流程 ---\nif __name__ == \"__main__\":\n    # 1. 加载文档数据 (只需要加载一次)\n    print(\"--- 加载文档数据 ---\")\n    documents_data = load_jsonl(DOCUMENTS_FILE)\n    if not documents_data:\n        print(\"错误：无法加载文档数据，评估中止。\")\n        exit()\n\n    # 2. 加载验证数据 (只需要加载一次)\n    print(\"\\n--- 加载验证数据 ---\")\n    val_data = load_jsonl(VAL_FILE)\n    if not val_data:\n        print(\"错误：无法加载验证数据，评估中止。\")\n        exit()\n\n    # 3. 初始化、构建索引并评估各个检索器\n    results = {}\n\n    # --- TF-IDF ---\n    tfidf_retriever = TfidfRetriever()\n    tfidf_retriever.build_index(documents_data)\n    if tfidf_retriever.tfidf_matrix is not None:\n        results['TF-IDF'] = evaluate_retriever(tfidf_retriever, val_data, top_n=5)\n    else:\n        results['TF-IDF'] = {'recall@5': '构建失败', 'mrr@5': '构建失败'}\n\n\n    # --- BM25 ---\n    bm25_retriever = Bm25Retriever()\n    bm25_retriever.build_index(documents_data)\n    if bm25_retriever.bm25_index is not None:\n         results['BM25'] = evaluate_retriever(bm25_retriever, val_data, top_n=5)\n    else:\n        results['BM25'] = {'recall@5': '构建失败', 'mrr@5': '构建失败'}\n\n    # --- FastText (词向量平均) ---\n    fasttext_retriever = FastTextRetriever(model_path=PRETRAINED_MODEL_PATH)\n    fasttext_retriever.load_model() # 加载模型\n    if fasttext_retriever.model is not None:\n        fasttext_retriever.build_index(documents_data) # 构建文档向量\n        if fasttext_retriever.doc_vectors is not None:\n            results['FastText Avg'] = evaluate_retriever(fasttext_retriever, val_data, top_n=5)\n        else:\n             results['FastText Avg'] = {'recall@5': '构建失败', 'mrr@5': '构建失败'}\n    else:\n         results['FastText Avg'] = {'recall@5': '模型加载失败', 'mrr@5': '模型加载失败'}\n\n\n    # 4. 打印结果表格\n    print(\"\\n\\n--- 检索器性能评估结果 (验证集) ---\")\n    print(\"-\" * 50)\n    print(f\"{'Retriever':<15} | {'Recall@5':<15} | {'MRR@5':<15}\")\n    print(\"-\" * 50)\n    for name, metrics in results.items():\n        recall_str = f\"{metrics.get('recall@5', 'N/A'):.4f}\" if isinstance(metrics.get('recall@5'), float) else str(metrics.get('recall@5', 'N/A'))\n        mrr_str = f\"{metrics.get('mrr@5', 'N/A'):.4f}\" if isinstance(metrics.get('mrr@5'), float) else str(metrics.get('mrr@5', 'N/A'))\n        print(f\"{name:<15} | {recall_str:<15} | {mrr_str:<15}\")\n    print(\"-\" * 50)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:57:48.084910Z","iopub.execute_input":"2025-04-05T15:57:48.085285Z","iopub.status.idle":"2025-04-05T16:38:35.167119Z","shell.execute_reply.started":"2025-04-05T15:57:48.085259Z","shell.execute_reply":"2025-04-05T16:38:35.165615Z"}},"outputs":[{"name":"stdout","text":"--- 加载文档数据 ---\n","output_type":"stream"},{"name":"stderr","text":"加载 documents.jsonl: 100%|██████████| 12138/12138 [00:04<00:00, 2969.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"成功加载 12138 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n\n--- 加载验证数据 ---\n","output_type":"stream"},{"name":"stderr","text":"加载 val.jsonl: 100%|██████████| 1000/1000 [00:00<00:00, 203864.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"成功加载 1000 条记录从 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n\n--- 构建 TF-IDF 索引 ---\n准备了 12138 个文档 ID 和 12138 个文档原文。\nTF-IDF 索引构建完成。矩阵形状: (12138, 279270). 耗时: 365.52 秒\n\n开始评估 TfidfRetriever (共 1000 个问题)...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating TfidfRetriever: 100%|██████████| 1000/1000 [12:43<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- 构建 BM25 索引 ---\n准备了 12138 个文档 ID 和 12138 个文档原文。\n正在对 12138 个文档进行预处理和分词...\n","output_type":"stream"},{"name":"stderr","text":"BM25 Preprocessing: 100%|██████████| 12138/12138 [05:38<00:00, 35.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n初始化 BM25Okapi...\nBM25 索引构建完成。耗时: 357.33 秒\n\n开始评估 Bm25Retriever (共 1000 个问题)...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Bm25Retriever: 100%|██████████| 1000/1000 [00:48<00:00, 20.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- 加载词向量模型 (crawl-300d-2M.vec) ---\n模型加载完成。耗时: 433.05 秒. V=1999995, D=300\n\n--- 构建文档向量索引 ---\n准备了 12138 个文档 ID 和 12138 个文档原文。\n正在对 12138 个文档进行预处理和向量化...\n","output_type":"stream"},{"name":"stderr","text":"Vectorizing Docs: 100%|██████████| 12138/12138 [07:39<00:00, 26.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"文档向量索引构建完成。矩阵形状: (12138, 300). 耗时: 459.70 秒\n\n开始评估 FastTextRetriever (共 1000 个问题)...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating FastTextRetriever: 100%|██████████| 1000/1000 [00:13<00:00, 74.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n\n--- 检索器性能评估结果 (验证集) ---\n--------------------------------------------------\nRetriever       | Recall@5        | MRR@5          \n--------------------------------------------------\nTF-IDF          | 0.6960          | 0.5123         \nBM25            | 0.7220          | 0.5538         \nFastText Avg    | 0.5570          | 0.3936         \n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install transformers torch faiss-gpu # 或者 faiss-cpu\n# 可能還需要 sentencepiece\n!pip install sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T02:53:01.096310Z","iopub.execute_input":"2025-04-06T02:53:01.096615Z","iopub.status.idle":"2025-04-06T02:53:12.638658Z","shell.execute_reply.started":"2025-04-06T02:53:01.096591Z","shell.execute_reply":"2025-04-06T02:53:12.637646Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nimport time\nimport os\n\n# --- 配置 ---\n# 選擇預訓練的 DPR 模型名稱 (針對 NQ 數據集微調過的)\n# Facebook 提供了針對 NQ 微調的模型\nQUESTION_ENCODER_NAME = 'facebook/dpr-question_encoder-single-nq-base'\nCONTEXT_ENCODER_NAME = 'facebook/dpr-ctx_encoder-single-nq-base'\n# 模型保存路徑 (可選，如果想將下載的模型保存在特定位置)\n# MODEL_SAVE_DIR = './dpr_models'\n# os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n\n# --- 檢查是否有可用的 GPU ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用的設備: {device}\")\n\n# --- 加載 DPR 模型和分詞器 ---\n\ndef load_dpr_models(q_encoder_name, ctx_encoder_name, device):\n    \"\"\"\n    從 Hugging Face Hub 加載 DPR 問題編碼器、上下文編碼器及對應的分詞器。\n\n    Args:\n        q_encoder_name (str): 問題編碼器模型名稱或路徑。\n        ctx_encoder_name (str): 上下文編碼器模型名稱或路徑。\n        device (torch.device): 模型運行的設備 (cpu 或 cuda)。\n\n    Returns:\n        tuple: 包含 (question_tokenizer, question_encoder, context_tokenizer, context_encoder) 的元組。\n               如果加載失敗，則對應項為 None。\n    \"\"\"\n    print(f\"開始加載 DPR 模型...\")\n    start_time = time.time()\n    try:\n        # 加載問題編碼器和分詞器\n        print(f\"  加載問題分詞器: {q_encoder_name}\")\n        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(q_encoder_name)\n        print(f\"  加載問題編碼器: {q_encoder_name}\")\n        q_encoder = DPRQuestionEncoder.from_pretrained(q_encoder_name)\n        q_encoder.to(device) # 將模型移動到指定設備\n        q_encoder.eval() # 設置為評估模式\n\n        # 加載上下文編碼器和分詞器\n        print(f\"  加載上下文分詞器: {ctx_encoder_name}\")\n        ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)\n        print(f\"  加載上下文編碼器: {ctx_encoder_name}\")\n        ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)\n        ctx_encoder.to(device) # 將模型移動到指定設備\n        ctx_encoder.eval() # 設置為評估模式\n\n        end_time = time.time()\n        print(f\"DPR 模型加載完成。耗時: {end_time - start_time:.2f} 秒\")\n        return q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder\n\n    except Exception as e:\n        print(f\"加載 DPR 模型時出錯: {e}\")\n        print(\"請確保模型名稱正確，網絡連接正常，或已安裝所需依賴。\")\n        return None, None, None, None\n\n# --- 主程序入口 ---\nif __name__ == \"__main__\":\n    # 執行模型加載\n    question_tokenizer, question_encoder, context_tokenizer, context_encoder = load_dpr_models(\n        QUESTION_ENCODER_NAME,\n        CONTEXT_ENCODER_NAME,\n        device\n    )\n\n    if question_encoder and context_encoder:\n        print(\"\\n成功加載所有 DPR 組件！\")\n        # 打印一些模型信息 (可選)\n        print(f\"問題編碼器配置: {question_encoder.config.model_type}, 隱藏層大小: {question_encoder.config.hidden_size}\")\n        print(f\"上下文編碼器配置: {context_encoder.config.model_type}, 隱藏層大小: {context_encoder.config.hidden_size}\")\n\n        # --- 接下來的步驟 (將在後續代碼中實現) ---\n        # 1. 加載 documents.jsonl\n        # 2. 遍歷文檔，使用 context_tokenizer 和 context_encoder 生成文檔向量\n        # 3. 使用 faiss 構建索引\n        # 4. 實現檢索函數 (接收問題，使用 question_tokenizer 和 question_encoder 生成問題向量，在 faiss 中搜索)\n        # 5. (可選) 進行評估\n        print(\"\\n下一步：使用加載的模型對文檔進行編碼並構建 FAISS 索引。\")\n\n    else:\n        print(\"\\nDPR 模型加載失敗，請檢查錯誤信息。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:58:33.751545Z","iopub.execute_input":"2025-04-05T16:58:33.751873Z","iopub.status.idle":"2025-04-05T16:59:02.739829Z","shell.execute_reply.started":"2025-04-05T16:58:33.751848Z","shell.execute_reply":"2025-04-05T16:59:02.738723Z"}},"outputs":[{"name":"stdout","text":"使用的設備: cuda\n開始加載 DPR 模型...\n  加載問題分詞器: facebook/dpr-question_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a10f72be9e14ccd9f25622b287b0fa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9cd802bea4c40c99a21adf4f5e1d739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c13a2a7b7b64dbfaf94dd1870793fd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e640a932cb647358a949e332595e6dd"}},"metadata":{}},{"name":"stdout","text":"  加載問題編碼器: facebook/dpr-question_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e35ac711df4f43a823c6a07780d032"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"  加載上下文分詞器: facebook/dpr-ctx_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e8376ba9c74db799107a31ece19360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41d1f22808b845a1b6b70197c63903f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8604bf268d040c8af5b709adf30a4a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e117db01c6450cb553ad5c5176d730"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"  加載上下文編碼器: facebook/dpr-ctx_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0d56c2528b415fa14ebab0211819ad"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"DPR 模型加載完成。耗時: 9.51 秒\n\n成功加載所有 DPR 組件！\n問題編碼器配置: dpr, 隱藏層大小: 768\n上下文編碼器配置: dpr, 隱藏層大小: 768\n\n下一步：使用加載的模型對文檔進行編碼並構建 FAISS 索引。\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nimport faiss # 引入 faiss\nimport numpy as np\nimport time\nimport os\nimport json\nfrom tqdm import tqdm\n\n# --- 配置 ---\nQUESTION_ENCODER_NAME = 'facebook/dpr-question_encoder-single-nq-base'\nCONTEXT_ENCODER_NAME = 'facebook/dpr-ctx_encoder-single-nq-base'\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data'\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\n# 保存 FAISS 索引和文檔 ID 映射的文件路徑\nFAISS_INDEX_PATH = \"dpr_faiss_index.idx\"\nDOC_IDS_PATH = \"dpr_doc_ids.json\"\n# 編碼時的批次大小 (根據你的內存調整，CPU 上可以設小一點)\nBATCH_SIZE = 32 # 例如 16, 32, 64\n# DPR 模型通常的最大序列長度\nMAX_LENGTH = 512\n\n# --- 檢查是否有可用的 GPU ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用的設備: {device}\")\n\n# --- 加載 DPR 模型和分詞器 (與上一步相同) ---\ndef load_dpr_models(q_encoder_name, ctx_encoder_name, device):\n    \"\"\"加載 DPR 模型。\"\"\"\n    print(f\"開始加載 DPR 模型...\")\n    start_time = time.time()\n    try:\n        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(q_encoder_name)\n        q_encoder = DPRQuestionEncoder.from_pretrained(q_encoder_name)\n        q_encoder.to(device).eval()\n\n        ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)\n        ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)\n        ctx_encoder.to(device).eval()\n\n        end_time = time.time()\n        print(f\"DPR 模型加載完成。耗時: {end_time - start_time:.2f} 秒\")\n        return q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder\n    except Exception as e:\n        print(f\"加載 DPR 模型時出錯: {e}\")\n        return None, None, None, None\n\n# --- 加載文檔數據 ---\ndef load_jsonl(file_path):\n    \"\"\"加載 JSON Lines 文件。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                 if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳過無法解析的行: {line.strip()} - 錯誤: {e}\")\n        print(f\"成功加載 {len(data)} 條記錄從 {file_path}\")\n    except Exception as e:\n        print(f\"加載文件時出錯 {file_path}: {e}\")\n    return data\n\n# --- 文檔編碼函數 ---\ndef encode_documents(documents, tokenizer, encoder, device, batch_size=32, max_length=512):\n    \"\"\"\n    使用 DPR 上下文編碼器對文檔進行編碼。\n\n    Args:\n        documents (list[dict]): 文檔數據列表，每個 dict 包含 'document_id' 和 'document_text'。\n        tokenizer: DPR 上下文分詞器。\n        encoder: DPR 上下文編碼器。\n        device: 運行設備。\n        batch_size (int): 批處理大小。\n        max_length (int): 分詞器最大長度。\n\n    Returns:\n        tuple: (numpy.ndarray of document vectors, list of document ids)\n               如果出錯則返回 (None, None)。\n    \"\"\"\n    doc_vectors = []\n    doc_ids = []\n    total_docs = len(documents)\n    print(f\"\\n開始對 {total_docs} 個文檔進行編碼 (Batch Size: {batch_size})...\")\n    print(\"這一步會非常耗時，尤其是在 CPU 上。\")\n\n    # 記錄所有 ID\n    for doc in documents:\n         if 'document_id' in doc:\n             doc_ids.append(doc['document_id'])\n\n    # 使用 tqdm 顯示進度\n    with tqdm(total=total_docs, desc=\"Encoding Documents\") as pbar:\n        for i in range(0, total_docs, batch_size):\n            batch_docs = documents[i : i + batch_size]\n            # 提取文本，處理可能的 None 或非字符串值\n            texts = [str(doc.get('document_text', '')) for doc in batch_docs]\n\n            try:\n                # 分詞\n                inputs = tokenizer(\n                    texts,\n                    max_length=max_length,\n                    padding='longest', # 在 batch 內部填充到最長\n                    truncation=True,\n                    return_tensors='pt'\n                )\n                # 將輸入移動到設備\n                inputs = {key: val.to(device) for key, val in inputs.items()}\n\n                # 在 no_grad 模式下進行編碼，節省內存和計算\n                with torch.no_grad():\n                    outputs = encoder(**inputs)\n                    # DPR 的輸出通常在 pooler_output 屬性中\n                    batch_vectors = outputs.pooler_output\n\n                # (推薦) L2 歸一化向量，以便使用 IndexFlatIP 計算余弦相似度\n                norms = torch.linalg.norm(batch_vectors, dim=1, keepdim=True)\n                normalized_vectors = batch_vectors / norms\n\n                # 將向量移回 CPU 並轉換為 NumPy 數組\n                doc_vectors.append(normalized_vectors.cpu().numpy())\n\n            except Exception as e:\n                print(f\"\\n處理批次 {i // batch_size} 時出錯: {e}\")\n                # 可以選擇跳過錯誤批次或中止\n                # continue\n                # return None, None\n\n            pbar.update(len(batch_docs))\n\n    if not doc_vectors:\n        print(\"錯誤：未能生成任何文檔向量。\")\n        return None, None\n\n    # 將所有批次的向量合併為一個大的 NumPy 數組\n    all_doc_vectors = np.concatenate(doc_vectors, axis=0)\n    print(f\"\\n文檔編碼完成。生成向量矩陣形狀: {all_doc_vectors.shape}\")\n    return all_doc_vectors.astype('float32'), doc_ids # FAISS 通常需要 float32\n\n# --- FAISS 索引構建函數 ---\ndef build_faiss_index(doc_vectors, index_path):\n    \"\"\"\n    使用文檔向量構建 FAISS 索引並保存。\n\n    Args:\n        doc_vectors (np.ndarray): 文檔向量 (float32)。\n        index_path (str): 保存索引的文件路徑。\n\n    Returns:\n        faiss.Index: 構建好的 FAISS 索引。如果出錯則返回 None。\n    \"\"\"\n    if doc_vectors is None or doc_vectors.shape[0] == 0:\n        print(\"錯誤：沒有有效的文檔向量來構建索引。\")\n        return None\n\n    vector_dim = doc_vectors.shape[1]\n    print(f\"\\n開始構建 FAISS 索引 (向量維度: {vector_dim})...\")\n    start_time = time.time()\n\n    # 選擇索引類型: IndexFlatIP (內積)。因為向量已歸一化，內積等價於余弦相似度。\n    # IndexFlatL2 計算歐氏距離。\n    index = faiss.IndexFlatIP(vector_dim)\n\n    # 添加向量到索引\n    print(f\"正在添加 {doc_vectors.shape[0]} 個向量到索引...\")\n    index.add(doc_vectors)\n\n    end_time = time.time()\n    print(f\"FAISS 索引構建完成。索引中文檔數: {index.ntotal}\")\n    print(f\"耗時: {end_time - start_time:.2f} 秒\")\n\n    # 保存索引\n    try:\n        print(f\"正在將索引保存到: {index_path}\")\n        faiss.write_index(index, index_path)\n        print(\"索引保存成功。\")\n    except Exception as e:\n        print(f\"保存 FAISS 索引時出錯: {e}\")\n\n    return index\n\n# --- 主程序入口 ---\nif __name__ == \"__main__\":\n    # 1. 加載 DPR 模型\n    question_tokenizer, question_encoder, context_tokenizer, context_encoder = load_dpr_models(\n        QUESTION_ENCODER_NAME,\n        CONTEXT_ENCODER_NAME,\n        device\n    )\n\n    faiss_index = None\n    doc_ids_list = []\n\n    # 檢查索引和 ID 映射是否已存在，如果存在則直接加載\n    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(DOC_IDS_PATH):\n        print(f\"\\n檢測到已存在的 FAISS 索引 ({FAISS_INDEX_PATH}) 和文檔 ID 映射 ({DOC_IDS_PATH})。\")\n        try:\n            print(\"正在加載 FAISS 索引...\")\n            faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n            print(f\"索引加載成功。包含 {faiss_index.ntotal} 個向量。\")\n\n            print(\"正在加載文檔 ID 映射...\")\n            with open(DOC_IDS_PATH, 'r') as f:\n                doc_ids_list = json.load(f)\n            print(f\"文檔 ID 映射加載成功。包含 {len(doc_ids_list)} 個 ID。\")\n\n            if faiss_index.ntotal != len(doc_ids_list):\n                print(\"警告：索引中的向量數與加載的 ID 數量不匹配！可能需要重新構建。\")\n                faiss_index = None # 標記為無效，觸發重新構建\n                doc_ids_list = []\n\n        except Exception as e:\n            print(f\"加載已存文件時出錯: {e}。將重新構建索引。\")\n            faiss_index = None\n            doc_ids_list = []\n\n    # 如果模型加載成功，並且索引未從文件加載，則執行編碼和構建\n    if context_encoder and context_tokenizer and faiss_index is None:\n        # 2. 加載文檔數據\n        print(\"\\n--- 加載文檔數據 ---\")\n        documents_data = load_jsonl(DOCUMENTS_FILE)\n\n        if documents_data:\n            # 3. 文檔編碼\n            document_vectors, doc_ids_list = encode_documents(\n                documents_data,\n                context_tokenizer,\n                context_encoder,\n                device,\n                batch_size=BATCH_SIZE,\n                max_length=MAX_LENGTH\n            )\n\n            if document_vectors is not None and doc_ids_list:\n                # 4. 構建 FAISS 索引\n                faiss_index = build_faiss_index(document_vectors, FAISS_INDEX_PATH)\n\n                # 5. 保存文檔 ID 列表\n                if faiss_index is not None: # 確保索引構建成功\n                    try:\n                        print(f\"正在將文檔 ID 列表保存到: {DOC_IDS_PATH}\")\n                        with open(DOC_IDS_PATH, 'w') as f:\n                            json.dump(doc_ids_list, f)\n                        print(\"文檔 ID 列表保存成功。\")\n                    except Exception as e:\n                        print(f\"保存文檔 ID 列表時出錯: {e}\")\n            else:\n                 print(\"文檔編碼失敗，無法構建索引。\")\n        else:\n            print(\"無法加載文檔數據，無法構建索引。\")\n\n    elif faiss_index is not None:\n         print(\"\\n已成功加載預先構建的 FAISS 索引和文檔 ID。\")\n\n    else:\n        print(\"\\nDPR 模型加載失敗 或 索引構建/加載失敗，無法繼續。\")\n\n\n    # --- 接下來的步驟 ---\n    if faiss_index is not None and question_encoder is not None and doc_ids_list:\n        print(\"\\n下一步：實現使用 FAISS 索引進行檢索的函數。\")\n        # 示例：獲取索引中的向量數量\n        print(f\"最終 FAISS 索引中的向量總數: {faiss_index.ntotal}\")\n        print(f\"對應的文檔 ID 數量: {len(doc_ids_list)}\")\n        # 實現 retrieve_dpr(query, top_n) -> list[doc_id]\n    else:\n        print(\"\\n未能成功準備好用於檢索的組件。\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:00:04.869492Z","iopub.execute_input":"2025-04-05T17:00:04.869811Z","iopub.status.idle":"2025-04-05T17:41:30.041479Z","shell.execute_reply.started":"2025-04-05T17:00:04.869788Z","shell.execute_reply":"2025-04-05T17:41:30.040448Z"}},"outputs":[{"name":"stdout","text":"使用的設備: cuda\n開始加載 DPR 模型...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizer'.\nSome weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"DPR 模型加載完成。耗時: 2.83 秒\n\n--- 加載文檔數據 ---\n成功加載 12138 條記錄從 /kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n\n開始對 12138 個文檔進行編碼 (Batch Size: 32)...\n這一步會非常耗時，尤其是在 CPU 上。\n","output_type":"stream"},{"name":"stderr","text":"Encoding Documents: 100%|██████████| 12138/12138 [41:13<00:00,  4.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n文檔編碼完成。生成向量矩陣形狀: (12138, 768)\n\n開始構建 FAISS 索引 (向量維度: 768)...\n正在添加 12138 個向量到索引...\nFAISS 索引構建完成。索引中文檔數: 12138\n耗時: 0.03 秒\n正在將索引保存到: dpr_faiss_index.idx\n索引保存成功。\n正在將文檔 ID 列表保存到: dpr_doc_ids.json\n文檔 ID 列表保存成功。\n\n下一步：實現使用 FAISS 索引進行檢索的函數。\n最終 FAISS 索引中的向量總數: 12138\n對應的文檔 ID 數量: 12138\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nimport faiss # 引入 faiss\nimport numpy as np\nimport time\nimport os\nimport json\nfrom tqdm import tqdm\n\n# --- 配置 ---\nQUESTION_ENCODER_NAME = 'facebook/dpr-question_encoder-single-nq-base'\nCONTEXT_ENCODER_NAME = 'facebook/dpr-ctx_encoder-single-nq-base'\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data'\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl') # 用於演示檢索\n# 保存/加載 FAISS 索引和文檔 ID 映射的文件路徑\nFAISS_INDEX_PATH = \"/kaggle/working/dpr_faiss_index.idx\"\nDOC_IDS_PATH = \"/kaggle/working/dpr_doc_ids.json\"\n# 編碼時的批次大小\nBATCH_SIZE = 32 # 文檔編碼時使用\n# DPR 模型通常的最大序列長度\nMAX_LENGTH = 512\n\n# --- 檢查是否有可用的 GPU ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用的設備: {device}\")\n\n# --- 加載 DPR 模型和分詞器 ---\ndef load_dpr_models(q_encoder_name, ctx_encoder_name, device):\n    \"\"\"加載 DPR 模型。\"\"\"\n    print(f\"開始加載 DPR 模型...\")\n    start_time = time.time()\n    try:\n        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(q_encoder_name)\n        q_encoder = DPRQuestionEncoder.from_pretrained(q_encoder_name)\n        q_encoder.to(device).eval()\n\n        # 注意：上下文分詞器僅在需要重新編碼文檔時才需要加載\n        # 如果索引已存在，可以考慮延遲加載或不加載 ctx_tokenizer, ctx_encoder\n        ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)\n        ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)\n        ctx_encoder.to(device).eval()\n\n        end_time = time.time()\n        print(f\"DPR 模型加載完成。耗時: {end_time - start_time:.2f} 秒\")\n        return q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder\n    except Exception as e:\n        print(f\"加載 DPR 模型時出錯: {e}\")\n        # 即使上下文編碼器加載失敗，如果索引存在，仍可能繼續進行檢索\n        # 但為了完整性，這裡返回 None\n        return None, None, None, None\n\n# --- 加載文檔/驗證數據 ---\ndef load_jsonl(file_path):\n    \"\"\"加載 JSON Lines 文件。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                 if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳過無法解析的行: {line.strip()} - 錯誤: {e}\")\n        print(f\"成功加載 {len(data)} 條記錄從 {file_path}\")\n    except Exception as e:\n        print(f\"加載文件時出錯 {file_path}: {e}\")\n    return data\n\n# --- 文檔編碼函數 ---\ndef encode_documents(documents, tokenizer, encoder, device, batch_size=32, max_length=512):\n    \"\"\"使用 DPR 上下文編碼器對文檔進行編碼。\"\"\"\n    doc_vectors = []\n    doc_ids = []\n    total_docs = len(documents)\n    print(f\"\\n開始對 {total_docs} 個文檔進行編碼 (Batch Size: {batch_size})...\")\n    print(\"這一步會非常耗時，尤其是在 CPU 上。\")\n    for doc in documents:\n         if 'document_id' in doc: doc_ids.append(doc['document_id'])\n    with tqdm(total=total_docs, desc=\"Encoding Documents\") as pbar:\n        for i in range(0, total_docs, batch_size):\n            batch_docs = documents[i : i + batch_size]\n            texts = [str(doc.get('document_text', '')) for doc in batch_docs]\n            try:\n                inputs = tokenizer(texts, max_length=max_length, padding='longest', truncation=True, return_tensors='pt')\n                inputs = {key: val.to(device) for key, val in inputs.items()}\n                with torch.no_grad():\n                    outputs = encoder(**inputs)\n                    batch_vectors = outputs.pooler_output\n                norms = torch.linalg.norm(batch_vectors, dim=1, keepdim=True)\n                # 添加一個小的 epsilon 防止除以零\n                normalized_vectors = batch_vectors / (norms + 1e-8)\n                doc_vectors.append(normalized_vectors.cpu().numpy())\n            except Exception as e:\n                print(f\"\\n處理批次 {i // batch_size} 時出錯: {e}\")\n            pbar.update(len(batch_docs))\n    if not doc_vectors: return None, None\n    all_doc_vectors = np.concatenate(doc_vectors, axis=0)\n    print(f\"\\n文檔編碼完成。生成向量矩陣形狀: {all_doc_vectors.shape}\")\n    return all_doc_vectors.astype('float32'), doc_ids\n\n# --- FAISS 索引構建函數 ---\ndef build_faiss_index(doc_vectors, index_path):\n    \"\"\"使用文檔向量構建 FAISS 索引並保存。\"\"\"\n    if doc_vectors is None or doc_vectors.shape[0] == 0: return None\n    vector_dim = doc_vectors.shape[1]\n    print(f\"\\n開始構建 FAISS 索引 (向量維度: {vector_dim})...\")\n    start_time = time.time()\n    index = faiss.IndexFlatIP(vector_dim)\n    print(f\"正在添加 {doc_vectors.shape[0]} 個向量到索引...\")\n    index.add(doc_vectors)\n    end_time = time.time()\n    print(f\"FAISS 索引構建完成。索引中文檔數: {index.ntotal}. 耗時: {end_time - start_time:.2f} 秒\")\n    try:\n        print(f\"正在將索引保存到: {index_path}\")\n        faiss.write_index(index, index_path)\n        print(\"索引保存成功。\")\n    except Exception as e: print(f\"保存 FAISS 索引時出錯: {e}\")\n    return index\n\n# --- 新增：DPR 檢索函數 ---\ndef retrieve_dpr(query, q_tokenizer, q_encoder, faiss_index, doc_ids_list, top_n, device):\n    \"\"\"\n    使用 DPR 問題編碼器和 FAISS 索引進行檢索。\n\n    Args:\n        query (str): 查詢問題。\n        q_tokenizer: DPR 問題分詞器。\n        q_encoder: DPR 問題編碼器。\n        faiss_index: 構建好的 FAISS 索引。\n        doc_ids_list (list): 與 FAISS 索引順序一致的文檔 ID 列表。\n        top_n (int): 需要檢索的 top N 結果。\n        device: 運行設備。\n\n    Returns:\n        list[int]: 檢索到的 top_n 個文檔 ID 列表。\n    \"\"\"\n    if not query or faiss_index is None or q_encoder is None or not doc_ids_list:\n        print(\"錯誤：檢索所需的組件不完整。\")\n        return []\n\n    try:\n        # 1. 編碼查詢\n        inputs = q_tokenizer(query, max_length=MAX_LENGTH, padding=False, truncation=True, return_tensors='pt')\n        inputs = {key: val.to(device) for key, val in inputs.items()}\n\n        with torch.no_grad():\n            outputs = q_encoder(**inputs)\n            query_vector = outputs.pooler_output\n\n        # 2. 歸一化查詢向量\n        norm = torch.linalg.norm(query_vector)\n        normalized_query_vector = query_vector / (norm + 1e-8) # 添加 epsilon\n\n        # 轉換為 NumPy float32 並 reshape\n        query_np = normalized_query_vector.cpu().numpy().astype('float32').reshape(1, -1)\n\n        # 3. 在 FAISS 中搜索\n        # search 返回 (距離/分數 D, 索引 I)\n        distances, indices = faiss_index.search(query_np, top_n)\n\n        # 4. 將 FAISS 索引映射回文檔 ID\n        retrieved_ids = [doc_ids_list[i] for i in indices[0] if i != -1] # i == -1 表示沒有找到足夠的鄰居\n\n        return retrieved_ids\n\n    except Exception as e:\n        print(f\"檢索過程中出錯: {e}\")\n        return []\n\n\n# --- 主程序入口 ---\nif __name__ == \"__main__\":\n    # 1. 加載 DPR 模型\n    # 只需要問題編碼器用於檢索，但如果需要重新構建索引，則兩者都需要\n    question_tokenizer, question_encoder, context_tokenizer, context_encoder = load_dpr_models(\n        QUESTION_ENCODER_NAME,\n        CONTEXT_ENCODER_NAME,\n        device\n    )\n\n    faiss_index = None\n    doc_ids_list = []\n\n    # 2. 嘗試加載已存在的索引和 ID 列表\n    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(DOC_IDS_PATH):\n        print(f\"\\n檢測到已存在的 FAISS 索引和文檔 ID 映射。\")\n        try:\n            print(\"正在加載 FAISS 索引...\")\n            faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n            print(f\"索引加載成功。包含 {faiss_index.ntotal} 個向量。\")\n            print(\"正在加載文檔 ID 映射...\")\n            with open(DOC_IDS_PATH, 'r') as f:\n                doc_ids_list = json.load(f)\n            print(f\"文檔 ID 映射加載成功。包含 {len(doc_ids_list)} 個 ID。\")\n            if faiss_index.ntotal != len(doc_ids_list):\n                print(\"警告：索引向量數與 ID 數量不匹配！\")\n                faiss_index = None; doc_ids_list = []\n        except Exception as e:\n            print(f\"加載已存文件時出錯: {e}。\")\n            faiss_index = None; doc_ids_list = []\n\n    # 3. 如果索引加載失敗或不存在，則嘗試構建\n    if faiss_index is None and context_encoder and context_tokenizer:\n        print(\"\\n需要構建 FAISS 索引...\")\n        print(\"\\n--- 加載文檔數據 ---\")\n        documents_data = load_jsonl(DOCUMENTS_FILE)\n        if documents_data:\n            document_vectors, doc_ids_list_build = encode_documents(\n                documents_data, context_tokenizer, context_encoder, device, BATCH_SIZE, MAX_LENGTH)\n            if document_vectors is not None and doc_ids_list_build:\n                faiss_index = build_faiss_index(document_vectors, FAISS_INDEX_PATH)\n                if faiss_index is not None:\n                    doc_ids_list = doc_ids_list_build # 使用新生成的 ID 列表\n                    try:\n                        print(f\"正在將新生成的文檔 ID 列表保存到: {DOC_IDS_PATH}\")\n                        with open(DOC_IDS_PATH, 'w') as f: json.dump(doc_ids_list, f)\n                        print(\"文檔 ID 列表保存成功。\")\n                    except Exception as e: print(f\"保存文檔 ID 列表時出錯: {e}\")\n            else: print(\"文檔編碼失敗，無法構建索引。\")\n        else: print(\"無法加載文檔數據，無法構建索引。\")\n\n    # 4. 檢查是否可以進行檢索\n    if faiss_index is not None and question_encoder is not None and question_tokenizer is not None and doc_ids_list:\n        print(\"\\n所有檢索組件準備就緒！\")\n\n        # 5. 演示檢索功能\n        print(\"\\n--- 演示 DPR 檢索 ---\")\n        # 加載驗證數據以獲取示例問題\n        val_data = load_jsonl(VAL_FILE)\n        if val_data:\n            example_index = 0 # 使用第一個驗證問題\n            example_question = val_data[example_index]['question']\n            true_doc_id = val_data[example_index]['document_id']\n\n            print(f\"示例問題 (來自 val.jsonl[{example_index}]): '{example_question}'\")\n            print(f\"真實相關的文檔 ID: {true_doc_id}\")\n\n            start_retrieval_time = time.time()\n            retrieved_ids = retrieve_dpr(\n                example_question,\n                question_tokenizer,\n                question_encoder,\n                faiss_index,\n                doc_ids_list,\n                top_n=5,\n                device=device\n            )\n            end_retrieval_time = time.time()\n\n            print(f\"\\nDPR 檢索到的 Top-5 文檔 ID:\")\n            print(retrieved_ids)\n            print(f\"檢索耗時: {end_retrieval_time - start_retrieval_time:.4f} 秒\")\n\n            # 檢查結果\n            if retrieved_ids and true_doc_id in retrieved_ids:\n                rank = retrieved_ids.index(true_doc_id) + 1\n                print(f\"成功! 真實文檔 ID {true_doc_id} 在檢索結果中排名第 {rank}。\")\n            elif retrieved_ids:\n                print(f\"失敗。真實文檔 ID {true_doc_id} 未在前 5 個檢索結果中。\")\n            else:\n                print(\"未能檢索到任何文檔。\")\n        else:\n            print(\"未能加載驗證數據進行演示。\")\n\n        print(\"\\n下一步：(可選) 使用驗證集評估 DPR 檢索性能，或開始實現答案生成。\")\n\n    else:\n        print(\"\\n未能成功準備好用於檢索的組件。請檢查之前的錯誤信息。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:48:25.415146Z","iopub.execute_input":"2025-04-05T17:48:25.415470Z","iopub.status.idle":"2025-04-05T17:48:27.839989Z","shell.execute_reply.started":"2025-04-05T17:48:25.415444Z","shell.execute_reply":"2025-04-05T17:48:27.838841Z"}},"outputs":[{"name":"stdout","text":"使用的設備: cuda\n開始加載 DPR 模型...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizer'.\nSome weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"DPR 模型加載完成。耗時: 2.31 秒\n\n檢測到已存在的 FAISS 索引和文檔 ID 映射。\n正在加載 FAISS 索引...\n索引加載成功。包含 12138 個向量。\n正在加載文檔 ID 映射...\n文檔 ID 映射加載成功。包含 12138 個 ID。\n\n所有檢索組件準備就緒！\n\n--- 演示 DPR 檢索 ---\n成功加載 1000 條記錄從 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n示例問題 (來自 val.jsonl[0]): 'when did the british first land in north america'\n真實相關的文檔 ID: 11484\n\nDPR 檢索到的 Top-5 文檔 ID:\n[11484, 4117, 81, 411, 7632]\n檢索耗時: 0.0242 秒\n成功! 真實文檔 ID 11484 在檢索結果中排名第 1。\n\n下一步：(可選) 使用驗證集評估 DPR 檢索性能，或開始實現答案生成。\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nimport faiss # 引入 faiss\nimport numpy as np\nimport time\nimport os\nimport json\nfrom tqdm import tqdm\n\n# --- 配置 ---\nQUESTION_ENCODER_NAME = 'facebook/dpr-question_encoder-single-nq-base'\nCONTEXT_ENCODER_NAME = 'facebook/dpr-ctx_encoder-single-nq-base'\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data'\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl') # 用於演示和評估\n# 保存/加載 FAISS 索引和文檔 ID 映射的文件路徑\nFAISS_INDEX_PATH = \"/kaggle/working/dpr_faiss_index.idx\"\nDOC_IDS_PATH = \"/kaggle/working/dpr_doc_ids.json\"\n# 編碼時的批次大小\nBATCH_SIZE = 32 # 文檔編碼時使用\n# DPR 模型通常的最大序列長度\nMAX_LENGTH = 512\n\n# --- 檢查是否有可用的 GPU ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用的設備: {device}\")\n\n# --- 加載 DPR 模型和分詞器 ---\ndef load_dpr_models(q_encoder_name, ctx_encoder_name, device):\n    \"\"\"加載 DPR 模型。\"\"\"\n    print(f\"開始加載 DPR 模型...\")\n    start_time = time.time()\n    # 初始化為 None\n    q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder = None, None, None, None\n    try:\n        print(f\"  加載問題分詞器: {q_encoder_name}\")\n        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(q_encoder_name)\n        print(f\"  加載問題編碼器: {q_encoder_name}\")\n        q_encoder = DPRQuestionEncoder.from_pretrained(q_encoder_name)\n        q_encoder.to(device).eval()\n\n        # 嘗試加載上下文模型，即使失敗也可能繼續（如果索引已存在）\n        try:\n            print(f\"  加載上下文分詞器: {ctx_encoder_name}\")\n            ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)\n            print(f\"  加載上下文編碼器: {ctx_encoder_name}\")\n            ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)\n            ctx_encoder.to(device).eval()\n        except Exception as e_ctx:\n             print(f\"警告：加載上下文模型時出錯（如果索引已存在，可能不影響檢索）: {e_ctx}\")\n\n        end_time = time.time()\n        print(f\"DPR 模型加載完成（或部分完成）。耗時: {end_time - start_time:.2f} 秒\")\n        return q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder\n    except Exception as e_q:\n        print(f\"加載 DPR 問題模型時出錯，無法繼續: {e_q}\")\n        return None, None, None, None\n\n\n# --- 加載文檔/驗證數據 ---\ndef load_jsonl(file_path):\n    \"\"\"加載 JSON Lines 文件。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                 if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳過無法解析的行: {line.strip()} - 錯誤: {e}\")\n        print(f\"成功加載 {len(data)} 條記錄從 {file_path}\")\n    except Exception as e:\n        print(f\"加載文件時出錯 {file_path}: {e}\")\n    return data\n\n# --- 文檔編碼函數 ---\ndef encode_documents(documents, tokenizer, encoder, device, batch_size=32, max_length=512):\n    \"\"\"使用 DPR 上下文編碼器對文檔進行編碼。\"\"\"\n    doc_vectors = []\n    doc_ids = []\n    total_docs = len(documents)\n    print(f\"\\n開始對 {total_docs} 個文檔進行編碼 (Batch Size: {batch_size})...\")\n    # print(\"這一步會非常耗時，尤其是在 CPU 上。\") # 已知信息\n    for doc in documents:\n         if 'document_id' in doc: doc_ids.append(doc['document_id'])\n    with tqdm(total=total_docs, desc=\"Encoding Documents\") as pbar:\n        for i in range(0, total_docs, batch_size):\n            batch_docs = documents[i : i + batch_size]\n            texts = [str(doc.get('document_text', '')) for doc in batch_docs]\n            try:\n                inputs = tokenizer(texts, max_length=max_length, padding='longest', truncation=True, return_tensors='pt')\n                inputs = {key: val.to(device) for key, val in inputs.items()}\n                with torch.no_grad():\n                    outputs = encoder(**inputs)\n                    batch_vectors = outputs.pooler_output\n                norms = torch.linalg.norm(batch_vectors, dim=1, keepdim=True)\n                normalized_vectors = batch_vectors / (norms + 1e-8)\n                doc_vectors.append(normalized_vectors.cpu().numpy())\n            except Exception as e:\n                print(f\"\\n處理批次 {i // batch_size} 時出錯: {e}\")\n            pbar.update(len(batch_docs))\n    if not doc_vectors: return None, None\n    all_doc_vectors = np.concatenate(doc_vectors, axis=0)\n    print(f\"\\n文檔編碼完成。生成向量矩陣形狀: {all_doc_vectors.shape}\")\n    return all_doc_vectors.astype('float32'), doc_ids\n\n# --- FAISS 索引構建函數 ---\ndef build_faiss_index(doc_vectors, index_path):\n    \"\"\"使用文檔向量構建 FAISS 索引並保存。\"\"\"\n    if doc_vectors is None or doc_vectors.shape[0] == 0: return None\n    vector_dim = doc_vectors.shape[1]\n    print(f\"\\n開始構建 FAISS 索引 (向量維度: {vector_dim})...\")\n    start_time = time.time()\n    index = faiss.IndexFlatIP(vector_dim)\n    print(f\"正在添加 {doc_vectors.shape[0]} 個向量到索引...\")\n    index.add(doc_vectors)\n    end_time = time.time()\n    print(f\"FAISS 索引構建完成。索引中文檔數: {index.ntotal}. 耗時: {end_time - start_time:.2f} 秒\")\n    try:\n        print(f\"正在將索引保存到: {index_path}\")\n        faiss.write_index(index, index_path)\n        print(\"索引保存成功。\")\n    except Exception as e: print(f\"保存 FAISS 索引時出錯: {e}\")\n    return index\n\n# --- 新增：DPR 檢索器類 ---\nclass DprFaissRetriever:\n    \"\"\"\n    封裝 DPR + FAISS 檢索邏輯的類。\n    \"\"\"\n    def __init__(self, q_tokenizer, q_encoder, faiss_index, doc_ids_list, device):\n        self.q_tokenizer = q_tokenizer\n        self.q_encoder = q_encoder\n        self.faiss_index = faiss_index\n        self.doc_ids_list = doc_ids_list\n        self.device = device\n        self.vector_dim = q_encoder.config.hidden_size if q_encoder else None\n\n    def retrieve(self, query, top_n=5):\n        \"\"\"\n        執行檢索。\n        \"\"\"\n        if not all([query, self.faiss_index, self.q_encoder, self.q_tokenizer, self.doc_ids_list, self.vector_dim]):\n            print(\"錯誤：DPR 檢索器初始化不完整或查詢無效。\")\n            return []\n\n        try:\n            # 1. 編碼查詢\n            inputs = self.q_tokenizer(query, max_length=MAX_LENGTH, padding=False, truncation=True, return_tensors='pt')\n            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n\n            with torch.no_grad():\n                outputs = self.q_encoder(**inputs)\n                query_vector = outputs.pooler_output\n\n            # 2. 歸一化查詢向量\n            norm = torch.linalg.norm(query_vector)\n            normalized_query_vector = query_vector / (norm + 1e-8)\n\n            # 轉換為 NumPy float32 並 reshape\n            query_np = normalized_query_vector.cpu().numpy().astype('float32').reshape(1, -1)\n\n            # 3. 在 FAISS 中搜索\n            distances, indices = self.faiss_index.search(query_np, top_n)\n\n            # 4. 將 FAISS 索引映射回文檔 ID\n            retrieved_ids = [self.doc_ids_list[i] for i in indices[0] if 0 <= i < len(self.doc_ids_list)] # 增加邊界檢查\n\n            return retrieved_ids\n\n        except Exception as e:\n            print(f\"檢索過程中出錯: {e}\")\n            return []\n\n# --- 評估函數 (與之前相同) ---\ndef evaluate_retriever(retriever, validation_data, top_n=5):\n    \"\"\"評估檢索器。\"\"\"\n    recall_sum = 0\n    mrr_sum = 0\n    total = len(validation_data)\n    if total == 0: return {f'recall@{top_n}': 0, f'mrr@{top_n}': 0}\n    print(f\"\\n開始評估 {type(retriever).__name__} (共 {total} 個問題)...\")\n    retrieved_results = [] # 用於存儲每次的檢索結果 (可選)\n    for item in tqdm(validation_data, desc=f\"Evaluating {type(retriever).__name__}\"):\n        question = item['question']\n        true_doc_id = item['document_id']\n        # 調用傳入的 retriever 對象的 retrieve 方法\n        retrieved_ids = retriever.retrieve(question, top_n=top_n)\n        retrieved_results.append(retrieved_ids) # 存儲結果\n        if true_doc_id in retrieved_ids:\n            recall_sum += 1\n            try:\n                rank = retrieved_ids.index(true_doc_id) + 1\n                mrr_sum += 1.0 / rank\n            except ValueError: pass\n    recall_at_n = recall_sum / total\n    mrr_at_n = mrr_sum / total\n    return {f'recall@{top_n}': recall_at_n, f'mrr@{top_n}': mrr_at_n} # 可以考慮同時返回 retrieved_results 用於分析\n\n# --- 主程序入口 ---\nif __name__ == \"__main__\":\n    # 1. 加載 DPR 模型\n    question_tokenizer, question_encoder, context_tokenizer, context_encoder = load_dpr_models(\n        QUESTION_ENCODER_NAME,\n        CONTEXT_ENCODER_NAME,\n        device\n    )\n\n    faiss_index = None\n    doc_ids_list = []\n\n    # 2. 嘗試加載已存在的索引和 ID 列表\n    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(DOC_IDS_PATH):\n        print(f\"\\n檢測到已存在的 FAISS 索引和文檔 ID 映射。\")\n        try:\n            print(\"正在加載 FAISS 索引...\")\n            faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n            print(f\"索引加載成功。包含 {faiss_index.ntotal} 個向量。\")\n            print(\"正在加載文檔 ID 映射...\")\n            with open(DOC_IDS_PATH, 'r') as f:\n                doc_ids_list = json.load(f)\n            print(f\"文檔 ID 映射加載成功。包含 {len(doc_ids_list)} 個 ID。\")\n            if faiss_index.ntotal != len(doc_ids_list):\n                print(\"警告：索引向量數與 ID 數量不匹配！\")\n                faiss_index = None; doc_ids_list = []\n        except Exception as e:\n            print(f\"加載已存文件時出錯: {e}。\")\n            faiss_index = None; doc_ids_list = []\n\n    # 3. 如果索引加載失敗或不存在，則嘗試構建\n    if faiss_index is None:\n        print(\"\\n需要構建 FAISS 索引...\")\n        if context_encoder and context_tokenizer: # 確保上下文模型已加載\n             print(\"\\n--- 加載文檔數據 ---\")\n             documents_data = load_jsonl(DOCUMENTS_FILE)\n             if documents_data:\n                 document_vectors, doc_ids_list_build = encode_documents(\n                     documents_data, context_tokenizer, context_encoder, device, BATCH_SIZE, MAX_LENGTH)\n                 if document_vectors is not None and doc_ids_list_build:\n                     faiss_index = build_faiss_index(document_vectors, FAISS_INDEX_PATH)\n                     if faiss_index is not None:\n                         doc_ids_list = doc_ids_list_build\n                         try:\n                             print(f\"正在將新生成的文檔 ID 列表保存到: {DOC_IDS_PATH}\")\n                             with open(DOC_IDS_PATH, 'w') as f: json.dump(doc_ids_list, f)\n                             print(\"文檔 ID 列表保存成功。\")\n                         except Exception as e: print(f\"保存文檔 ID 列表時出錯: {e}\")\n                 else: print(\"文檔編碼失敗，無法構建索引。\")\n             else: print(\"無法加載文檔數據，無法構建索引。\")\n        else:\n             print(\"錯誤：上下文編碼器未成功加載，無法構建新的 FAISS 索引。\")\n\n\n    # 4. 檢查是否可以進行評估\n    if faiss_index is not None and question_encoder is not None and question_tokenizer is not None and doc_ids_list:\n        print(\"\\n所有 DPR 檢索組件準備就緒！\")\n\n        # 5. 加載驗證數據\n        print(\"\\n--- 加載驗證數據 ---\")\n        val_data = load_jsonl(VAL_FILE)\n\n        if val_data:\n             # 6. 創建 DPR 檢索器實例\n             dpr_retriever = DprFaissRetriever(\n                 question_tokenizer,\n                 question_encoder,\n                 faiss_index,\n                 doc_ids_list,\n                 device\n             )\n\n             # 7. 執行評估\n             results = {}\n             # 運行 DPR 評估\n             results['DPR + FAISS'] = evaluate_retriever(dpr_retriever, val_data, top_n=5)\n\n             # (可選) 重新運行之前的基線評估進行比較\n             # print(\"\\n--- 重新運行基線評估 (可選) ---\")\n             # tfidf_retriever = TfidfRetriever()\n             # tfidf_retriever.build_index(load_jsonl(DOCUMENTS_FILE)) # 需要重新加載文檔\n             # if tfidf_retriever.tfidf_matrix is not None:\n             #     results['TF-IDF'] = evaluate_retriever(tfidf_retriever, val_data, top_n=5)\n             # bm25_retriever = Bm25Retriever()\n             # bm25_retriever.build_index(load_jsonl(DOCUMENTS_FILE))\n             # if bm25_retriever.bm25_index is not None:\n             #     results['BM25'] = evaluate_retriever(bm25_retriever, val_data, top_n=5)\n\n\n             # 8. 打印結果表格\n             print(\"\\n\\n--- DPR 檢索器性能評估結果 (驗證集) ---\")\n             print(\"-\" * 60)\n             print(f\"{'Retriever':<25} | {'Recall@5':<15} | {'MRR@5':<15}\")\n             print(\"-\" * 60)\n             # 僅打印 DPR 結果，或取消註釋上面基線部分以顯示完整比較\n             order = ['DPR + FAISS'] #, 'TF-IDF', 'BM25']\n             for name in order:\n                 if name in results:\n                     metrics = results[name]\n                     recall_str = f\"{metrics.get('recall@5', 'N/A'):.4f}\" if isinstance(metrics.get('recall@5'), float) else str(metrics.get('recall@5', 'N/A'))\n                     mrr_str = f\"{metrics.get('mrr@5', 'N/A'):.4f}\" if isinstance(metrics.get('mrr@5'), float) else str(metrics.get('mrr@5', 'N/A'))\n                     print(f\"{name:<25} | {recall_str:<15} | {mrr_str:<15}\")\n             print(\"-\" * 60)\n\n             print(\"\\n下一步：實現答案生成。\")\n\n        else:\n            print(\"未能加載驗證數據進行評估。\")\n\n    else:\n        print(\"\\n未能成功準備好用於 DPR 評估的組件。請檢查之前的錯誤信息。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T02:53:25.231215Z","iopub.execute_input":"2025-04-06T02:53:25.231559Z","iopub.status.idle":"2025-04-06T02:53:34.536435Z","shell.execute_reply.started":"2025-04-06T02:53:25.231523Z","shell.execute_reply":"2025-04-06T02:53:34.534991Z"}},"outputs":[{"name":"stdout","text":"使用的設備: cuda\n開始加載 DPR 模型...\n  加載問題分詞器: facebook/dpr-question_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9401456a0e804d20baf377479da2afb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ade3253b4d14f06a28b7983f4966200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a80fe97a6ea484c990721c9d12a6fab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa0aa052bf99406eababff003d712004"}},"metadata":{}},{"name":"stdout","text":"  加載問題編碼器: facebook/dpr-question_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02754ae4f91448a8a1caf1c3b264d036"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"  加載上下文分詞器: facebook/dpr-ctx_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fdf75959e60454b859e90873b6cd74b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387468e607fd48bf818a257d498f388a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1253b69722f245de8687915e1e6e1b8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce989effc954075b98dd809810d8f2d"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"  加載上下文編碼器: facebook/dpr-ctx_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efae008d382e4d3a90f480530044f6a5"}},"metadata":{}},{"name":"stderr","text":"Process Process-auto_conversion:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/safetensors_conversion.py\", line 93, in auto_conversion\n    sharded = api.file_exists(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 2885, in file_exists\n    get_hf_file_metadata(url, token=token)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 280, in _request_wrapper\n    response = _request_wrapper(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 303, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 96, in send\n    return super().send(request, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n    response = conn.getresponse()\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 516, in getresponse\n    httplib_response = super().getresponse()\n  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n    response.begin()\n  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n    version, status, reason = self._read_status()\n  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\n  File \"/usr/lib/python3.10/ssl.py\", line 1303, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/usr/lib/python3.10/ssl.py\", line 1159, in read\n    return self._sslobj.read(len, buffer)\nKeyboardInterrupt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-642d7a988270>\u001b[0m in \u001b[0;36m<cell line: 206>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# 1. 加載 DPR 模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     question_tokenizer, question_encoder, context_tokenizer, context_encoder = load_dpr_models(\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mQUESTION_ENCODER_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mCONTEXT_ENCODER_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-642d7a988270>\u001b[0m in \u001b[0;36mload_dpr_models\u001b[0;34m(q_encoder_name, ctx_encoder_name, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mctx_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDPRContextEncoderTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_encoder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  加載上下文編碼器: {ctx_encoder_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mctx_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDPRContextEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_encoder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mctx_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3852\u001b[0m                             \u001b[0;31m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m                             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_add_variant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3854\u001b[0;31m                             resolved_archive_file = cached_file(\n\u001b[0m\u001b[1;32m   3855\u001b[0m                                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3856\u001b[0m                             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".incomplete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m         http_get(\n\u001b[0m\u001b[1;32m   1548\u001b[0m             \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     def _raw_read(\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nimport faiss # 引入 faiss\nimport numpy as np\nimport time\nimport os\nimport json\nfrom tqdm import tqdm\nimport math\nimport re # 用於簡單清理\nimport nltk # 用於分詞 (如果需要更精確的詞數分割)\n\n# --- 配置 ---\nQUESTION_ENCODER_NAME = 'facebook/dpr-question_encoder-single-nq-base'\nCONTEXT_ENCODER_NAME = 'facebook/dpr-ctx_encoder-single-nq-base'\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data'\nDOCUMENTS_FILE = os.path.join(DATA_DIR, 'documents.jsonl')\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl') # 用於評估\n\n# --- 修改：段落索引和映射的文件路徑 ---\nPASSAGE_FAISS_INDEX_PATH = \"dpr_passage_faiss_index.idx\"\nPASSAGE_MAPPING_PATH = \"dpr_passage_mapping.json\" # 存儲索引位置 -> (原始文檔ID, 段落文本/ID)\n\n# 編碼時的批次大小\nBATCH_SIZE = 32\n# DPR 模型最大序列長度\nMAX_LENGTH = 512\n# --- 新增：文檔分塊參數 ---\nCHUNK_SIZE = 100 # 每個段落的目標詞數\nCHUNK_OVERLAP = 20 # 段落間重疊的詞數\n\n# --- 檢查是否有可用的 GPU ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用的設備: {device}\")\n\n# --- NLTK數據檢查 (用於分詞) ---\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"下載 NLTK 'punkt' 數據...\")\n    nltk.download('punkt', quiet=True)\n\n# --- 加載 DPR 模型 (與之前類似) ---\ndef load_dpr_models(q_encoder_name, ctx_encoder_name, device):\n    \"\"\"加載 DPR 模型。\"\"\"\n    print(f\"開始加載 DPR 模型...\")\n    start_time = time.time()\n    q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder = None, None, None, None\n    try:\n        print(f\"  加載問題分詞器: {q_encoder_name}\")\n        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(q_encoder_name)\n        print(f\"  加載問題編碼器: {q_encoder_name}\")\n        q_encoder = DPRQuestionEncoder.from_pretrained(q_encoder_name)\n        q_encoder.to(device).eval()\n        try:\n            print(f\"  加載上下文分詞器: {ctx_encoder_name}\")\n            ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)\n            print(f\"  加載上下文編碼器: {ctx_encoder_name}\")\n            ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)\n            ctx_encoder.to(device).eval()\n        except Exception as e_ctx:\n             print(f\"警告：加載上下文模型時出錯: {e_ctx}\")\n        end_time = time.time()\n        print(f\"DPR 模型加載完成（或部分完成）。耗時: {end_time - start_time:.2f} 秒\")\n        return q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder\n    except Exception as e_q:\n        print(f\"加載 DPR 問題模型時出錯，無法繼續: {e_q}\")\n        return None, None, None, None\n\n# --- 加載數據 (與之前相同) ---\ndef load_jsonl(file_path):\n    \"\"\"加載 JSON Lines 文件。\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        print(f\"警告: 文件未找到 {file_path}\")\n        return data\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                 if line.strip():\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        print(f\"警告: 跳過無法解析的行: {line.strip()} - 錯誤: {e}\")\n        print(f\"成功加載 {len(data)} 條記錄從 {file_path}\")\n    except Exception as e:\n        print(f\"加載文件時出錯 {file_path}: {e}\")\n    return data\n\n# --- 新增：文檔分塊函數 ---\ndef split_document(doc_text, chunk_size, overlap):\n    \"\"\"\n    將文檔文本按詞數分割成重疊的段落。\n    使用 nltk.word_tokenize 進行分詞。\n    \"\"\"\n    if not isinstance(doc_text, str) or not doc_text.strip():\n        return []\n    # 簡單清理：移除多餘空白\n    text = re.sub(r'\\s+', ' ', doc_text).strip()\n    # 使用 nltk 分詞\n    try:\n        tokens = nltk.word_tokenize(text)\n    except Exception as e:\n        print(f\"分詞錯誤: {e}, 使用簡單空格分割替代。\")\n        tokens = text.split() # 回退機制\n\n    if not tokens:\n        return []\n\n    passages = []\n    stride = chunk_size - overlap # 每次滑動的步長\n    if stride <= 0: # 確保步長為正\n        stride = max(1, chunk_size // 2)\n        print(f\"警告：重疊過大，調整步長為 {stride}\")\n\n    for i in range(0, len(tokens), stride):\n        chunk = tokens[i : i + chunk_size]\n        if not chunk: continue\n        # 將 token 列表重新組合成字符串\n        passages.append(\" \".join(chunk))\n        # 如果最後一個 chunk 已包含所有剩餘 token，則停止\n        if i + chunk_size >= len(tokens):\n            break\n    return passages\n\n\n# --- 修改：段落編碼函數 ---\ndef encode_passages(documents, tokenizer, encoder, device, batch_size=32, max_length=512, chunk_size=100, overlap=20):\n    \"\"\"\n    對文檔進行分塊，並使用 DPR 上下文編碼器對段落進行編碼。\n\n    Args:\n        documents (list[dict]): 原始文檔數據列表。\n        tokenizer, encoder, device, batch_size, max_length: 同之前。\n        chunk_size (int): 段落目標詞數。\n        overlap (int): 段落重疊詞數。\n\n    Returns:\n        tuple: (numpy.ndarray of passage vectors, list of passage mappings)\n               passage_mappings: [(原始文檔ID, 段落文本), ...]\n               如果出錯則返回 (None, None)。\n    \"\"\"\n    all_passages_info = [] # 存儲 (原始文檔ID, 段落文本)\n    print(f\"\\n開始對 {len(documents)} 個文檔進行分塊 (Chunk Size: {chunk_size}, Overlap: {overlap})...\")\n    for doc in tqdm(documents, desc=\"Splitting Documents\"):\n        doc_id = doc.get('document_id')\n        doc_text = doc.get('document_text', '')\n        if doc_id is None or not doc_text:\n            continue\n        passages = split_document(doc_text, chunk_size, overlap)\n        for passage_text in passages:\n            all_passages_info.append((doc_id, passage_text)) # 記錄來源文檔ID和段落文本\n\n    if not all_passages_info:\n        print(\"錯誤：未能從文檔中生成任何段落。\")\n        return None, None\n\n    total_passages = len(all_passages_info)\n    print(f\"\\n共生成 {total_passages} 個段落。開始對段落進行編碼 (Batch Size: {batch_size})...\")\n\n    passage_vectors = []\n    passage_mapping_for_index = [] # 存儲與向量順序一致的 (原始文檔ID, 段落文本)\n\n    with tqdm(total=total_passages, desc=\"Encoding Passages\") as pbar:\n        for i in range(0, total_passages, batch_size):\n            batch_info = all_passages_info[i : i + batch_size]\n            batch_texts = [info[1] for info in batch_info] # 提取段落文本\n\n            try:\n                inputs = tokenizer(batch_texts, max_length=max_length, padding='longest', truncation=True, return_tensors='pt')\n                inputs = {key: val.to(device) for key, val in inputs.items()}\n                with torch.no_grad():\n                    outputs = encoder(**inputs)\n                    batch_vectors = outputs.pooler_output\n                norms = torch.linalg.norm(batch_vectors, dim=1, keepdim=True)\n                normalized_vectors = batch_vectors / (norms + 1e-8)\n                passage_vectors.append(normalized_vectors.cpu().numpy())\n                # 將當前批次的信息加入映射列表\n                passage_mapping_for_index.extend(batch_info)\n\n            except Exception as e:\n                print(f\"\\n處理段落批次 {i // batch_size} 時出錯: {e}\")\n            pbar.update(len(batch_info))\n\n    if not passage_vectors:\n        print(\"錯誤：未能生成任何段落向量。\")\n        return None, None\n\n    all_passage_vectors = np.concatenate(passage_vectors, axis=0)\n    print(f\"\\n段落編碼完成。生成向量矩陣形狀: {all_passage_vectors.shape}\")\n    # 確保向量數量和映射信息數量一致\n    if all_passage_vectors.shape[0] != len(passage_mapping_for_index):\n        print(f\"警告：向量數量 ({all_passage_vectors.shape[0]}) 與映射信息數量 ({len(passage_mapping_for_index)}) 不匹配！\")\n        # 可以選擇截斷或報錯\n        min_len = min(all_passage_vectors.shape[0], len(passage_mapping_for_index))\n        all_passage_vectors = all_passage_vectors[:min_len]\n        passage_mapping_for_index = passage_mapping_for_index[:min_len]\n        print(f\"已截斷至最小長度: {min_len}\")\n\n    return all_passage_vectors.astype('float32'), passage_mapping_for_index\n\n# --- 修改：FAISS 索引構建函數 (名稱變化) ---\ndef build_passage_faiss_index(passage_vectors, index_path):\n    \"\"\"使用段落向量構建 FAISS 索引並保存。\"\"\"\n    if passage_vectors is None or passage_vectors.shape[0] == 0: return None\n    vector_dim = passage_vectors.shape[1]\n    print(f\"\\n開始構建段落 FAISS 索引 (向量維度: {vector_dim})...\")\n    start_time = time.time()\n    index = faiss.IndexFlatIP(vector_dim)\n    print(f\"正在添加 {passage_vectors.shape[0]} 個段落向量到索引...\")\n    index.add(passage_vectors)\n    end_time = time.time()\n    print(f\"段落 FAISS 索引構建完成。索引中段落數: {index.ntotal}. 耗時: {end_time - start_time:.2f} 秒\")\n    try:\n        print(f\"正在將索引保存到: {index_path}\")\n        faiss.write_index(index, index_path)\n        print(\"索引保存成功。\")\n    except Exception as e: print(f\"保存 FAISS 索引時出錯: {e}\")\n    return index\n\n# --- 修改：DPR 段落檢索器類 ---\nclass DprPassageFaissRetriever:\n    \"\"\"\n    封裝 DPR (段落級) + FAISS 檢索邏輯的類。\n    \"\"\"\n    def __init__(self, q_tokenizer, q_encoder, faiss_index, passage_mapping, device):\n        \"\"\"\n        Args:\n            passage_mapping (list): 列表，每個元素是 (原始文檔ID, 段落文本)，順序與 FAISS 索引一致。\n        \"\"\"\n        self.q_tokenizer = q_tokenizer\n        self.q_encoder = q_encoder\n        self.faiss_index = faiss_index\n        self.passage_mapping = passage_mapping # 存儲 (原始文檔ID, 段落文本) 列表\n        self.device = device\n        self.vector_dim = q_encoder.config.hidden_size if q_encoder else None\n\n    def retrieve_passages(self, query, top_k):\n        \"\"\"檢索 top_k 個最相關的段落及其得分和來源文檔ID。\"\"\"\n        if not all([query, self.faiss_index, self.q_encoder, self.q_tokenizer, self.passage_mapping, self.vector_dim]):\n            return [], []\n\n        try:\n            inputs = self.q_tokenizer(query, max_length=MAX_LENGTH, padding=False, truncation=True, return_tensors='pt')\n            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n            with torch.no_grad():\n                outputs = self.q_encoder(**inputs)\n                query_vector = outputs.pooler_output\n            norm = torch.linalg.norm(query_vector)\n            normalized_query_vector = query_vector / (norm + 1e-8)\n            query_np = normalized_query_vector.cpu().numpy().astype('float32').reshape(1, -1)\n\n            # 搜索 FAISS 獲取段落索引和得分（內積）\n            scores, indices = self.faiss_index.search(query_np, top_k)\n\n            results = []\n            valid_indices = indices[0]\n            valid_scores = scores[0]\n\n            for i, idx in enumerate(valid_indices):\n                if 0 <= idx < len(self.passage_mapping):\n                    original_doc_id, passage_text = self.passage_mapping[idx]\n                    results.append({\n                        \"doc_id\": original_doc_id,\n                        \"passage_text\": passage_text, # 可以包含段落文本用於後續處理\n                        \"score\": float(valid_scores[i]),\n                        \"passage_index_in_faiss\": int(idx) # 記錄在faiss中的原始索引\n                    })\n            return results\n\n        except Exception as e:\n            print(f\"檢索段落過程中出錯: {e}\")\n            return []\n\n\n    def retrieve(self, query, top_n=5):\n        \"\"\"\n        執行檢索，返回 top_n 個最相關的 *文檔* ID。\n        策略：檢索更多段落，按文檔聚合，取每個文檔最高得分，再排序。\n        \"\"\"\n        # 檢索更多的段落，例如 top 20 或 50，以增加覆蓋到 top 5 文檔的可能性\n        top_k_passages = 20\n        passage_results = self.retrieve_passages(query, top_k=top_k_passages)\n\n        if not passage_results:\n            return []\n\n        # 按文檔 ID 聚合，記錄每個文檔的最高得分\n        doc_scores = {}\n        for p in passage_results:\n            doc_id = p[\"doc_id\"]\n            score = p[\"score\"]\n            if doc_id not in doc_scores or score > doc_scores[doc_id]:\n                doc_scores[doc_id] = score\n\n        # 按最高得分對文檔 ID 進行排序\n        sorted_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n\n        # 返回 top_n 個文檔 ID\n        top_n_doc_ids = [doc_id for doc_id, score in sorted_docs[:top_n]]\n\n        return top_n_doc_ids\n\n# --- 評估函數 (與之前相同) ---\ndef evaluate_retriever(retriever, validation_data, top_n=5):\n    \"\"\"評估檢索器。\"\"\"\n    recall_sum = 0\n    mrr_sum = 0\n    total = len(validation_data)\n    if total == 0: return {f'recall@{top_n}': 0, f'mrr@{top_n}': 0}\n    print(f\"\\n開始評估 {type(retriever).__name__} (共 {total} 個問題)...\")\n    for item in tqdm(validation_data, desc=f\"Evaluating {type(retriever).__name__}\"):\n        question = item['question']\n        true_doc_id = item['document_id']\n        retrieved_ids = retriever.retrieve(question, top_n=top_n) # 這裡返回的是文檔 ID\n        if true_doc_id in retrieved_ids:\n            recall_sum += 1\n            try:\n                rank = retrieved_ids.index(true_doc_id) + 1\n                mrr_sum += 1.0 / rank\n            except ValueError: pass\n    recall_at_n = recall_sum / total\n    mrr_at_n = mrr_sum / total\n    return {f'recall@{top_n}': recall_at_n, f'mrr@{top_n}': mrr_at_n}\n\n# --- 主程序入口 ---\nif __name__ == \"__main__\":\n    # 1. 加載 DPR 模型\n    question_tokenizer, question_encoder, context_tokenizer, context_encoder = load_dpr_models(\n        QUESTION_ENCODER_NAME,\n        CONTEXT_ENCODER_NAME,\n        device\n    )\n\n    passage_faiss_index = None\n    passage_mapping = [] # 存儲 (doc_id, passage_text)\n\n    # 2. 嘗試加載已存在的段落索引和映射\n    if os.path.exists(PASSAGE_FAISS_INDEX_PATH) and os.path.exists(PASSAGE_MAPPING_PATH):\n        print(f\"\\n檢測到已存在的段落 FAISS 索引和映射文件。\")\n        try:\n            print(\"正在加載段落 FAISS 索引...\")\n            passage_faiss_index = faiss.read_index(PASSAGE_FAISS_INDEX_PATH)\n            print(f\"索引加載成功。包含 {passage_faiss_index.ntotal} 個段落向量。\")\n            print(\"正在加載段落映射...\")\n            with open(PASSAGE_MAPPING_PATH, 'r') as f:\n                passage_mapping = json.load(f) # 加載 [(doc_id, passage_text), ...]\n            print(f\"段落映射加載成功。包含 {len(passage_mapping)} 個條目。\")\n            if passage_faiss_index.ntotal != len(passage_mapping):\n                print(\"警告：索引向量數與映射數量不匹配！\")\n                passage_faiss_index = None; passage_mapping = []\n        except Exception as e:\n            print(f\"加載已存文件時出錯: {e}。\")\n            passage_faiss_index = None; passage_mapping = []\n\n    # 3. 如果索引加載失敗或不存在，則嘗試構建\n    if passage_faiss_index is None:\n        print(\"\\n需要構建段落 FAISS 索引...\")\n        if context_encoder and context_tokenizer:\n             print(\"\\n--- 加載原始文檔數據 ---\")\n             documents_data = load_jsonl(DOCUMENTS_FILE)\n             if documents_data:\n                 # 執行段落編碼\n                 passage_vectors, passage_mapping_build = encode_passages(\n                     documents_data, context_tokenizer, context_encoder, device,\n                     BATCH_SIZE, MAX_LENGTH, CHUNK_SIZE, CHUNK_OVERLAP)\n\n                 if passage_vectors is not None and passage_mapping_build:\n                     # 構建段落 FAISS 索引\n                     passage_faiss_index = build_passage_faiss_index(passage_vectors, PASSAGE_FAISS_INDEX_PATH)\n                     if passage_faiss_index is not None:\n                         passage_mapping = passage_mapping_build # 使用新生成的映射\n                         try:\n                             print(f\"正在將新生成的段落映射保存到: {PASSAGE_MAPPING_PATH}\")\n                             with open(PASSAGE_MAPPING_PATH, 'w') as f: json.dump(passage_mapping, f)\n                             print(\"段落映射保存成功。\")\n                         except Exception as e: print(f\"保存段落映射時出錯: {e}\")\n                 else: print(\"段落編碼失敗，無法構建索引。\")\n             else: print(\"無法加載文檔數據，無法構建索引。\")\n        else:\n             print(\"錯誤：上下文編碼器未成功加載，無法構建新的 FAISS 索引。\")\n\n\n    # 4. 檢查是否可以進行評估\n    if passage_faiss_index is not None and question_encoder is not None and question_tokenizer is not None and passage_mapping:\n        print(\"\\n所有 DPR (段落級) 檢索組件準備就緒！\")\n\n        # 5. 加載驗證數據\n        print(\"\\n--- 加載驗證數據 ---\")\n        val_data = load_jsonl(VAL_FILE)\n\n        if val_data:\n             # 6. 創建 DPR 段落檢索器實例\n             dpr_passage_retriever = DprPassageFaissRetriever(\n                 question_tokenizer,\n                 question_encoder,\n                 passage_faiss_index,\n                 passage_mapping, # 傳遞段落映射\n                 device\n             )\n\n             # 7. 執行評估\n             results = {}\n             results['DPR Passage'] = evaluate_retriever(dpr_passage_retriever, val_data, top_n=5)\n\n             # 8. 打印結果表格\n             print(\"\\n\\n--- DPR (段落級) 檢索器性能評估結果 (驗證集) ---\")\n             print(\"-\" * 60)\n             print(f\"{'Retriever':<25} | {'Recall@5':<15} | {'MRR@5':<15}\")\n             print(\"-\" * 60)\n             order = ['DPR Passage']\n             for name in order:\n                 if name in results:\n                     metrics = results[name]\n                     recall_str = f\"{metrics.get('recall@5', 'N/A'):.4f}\" if isinstance(metrics.get('recall@5'), float) else str(metrics.get('recall@5', 'N/A'))\n                     mrr_str = f\"{metrics.get('mrr@5', 'N/A'):.4f}\" if isinstance(metrics.get('mrr@5'), float) else str(metrics.get('mrr@5', 'N/A'))\n                     print(f\"{name:<25} | {recall_str:<15} | {mrr_str:<15}\")\n             print(\"-\" * 60)\n\n             print(\"\\n下一步：實現答案生成。\")\n\n        else:\n            print(\"未能加載驗證數據進行評估。\")\n\n    else:\n        print(\"\\n未能成功準備好用於 DPR (段落級) 評估的組件。請檢查之前的錯誤信息。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T02:53:46.510713Z","iopub.execute_input":"2025-04-06T02:53:46.511109Z","iopub.status.idle":"2025-04-06T06:52:51.691765Z","shell.execute_reply.started":"2025-04-06T02:53:46.511072Z","shell.execute_reply":"2025-04-06T06:52:51.689129Z"}},"outputs":[{"name":"stdout","text":"使用的設備: cuda\n開始加載 DPR 模型...\n  加載問題分詞器: facebook/dpr-question_encoder-single-nq-base\n  加載問題編碼器: facebook/dpr-question_encoder-single-nq-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"  加載上下文分詞器: facebook/dpr-ctx_encoder-single-nq-base\n  加載上下文編碼器: facebook/dpr-ctx_encoder-single-nq-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:  55%|#####5    | 241M/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d8b7b50a0f45d88a7798c2ad2a00bc"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"DPR 模型加載完成（或部分完成）。耗時: 4.66 秒\n\n需要構建段落 FAISS 索引...\n\n--- 加載原始文檔數據 ---\n成功加載 12138 條記錄從 /kaggle/input/nq10k-comp5423/data_and_code/data/documents.jsonl\n\n開始對 12138 個文檔進行分塊 (Chunk Size: 100, Overlap: 20)...\n","output_type":"stream"},{"name":"stderr","text":"Splitting Documents: 100%|██████████| 12138/12138 [13:02<00:00, 15.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n共生成 1932676 個段落。開始對段落進行編碼 (Batch Size: 32)...\n","output_type":"stream"},{"name":"stderr","text":"Encoding Passages: 100%|██████████| 1932676/1932676 [3:11:10<00:00, 168.50it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\n段落編碼完成。生成向量矩陣形狀: (1932676, 768)\n\n開始構建段落 FAISS 索引 (向量維度: 768)...\n正在添加 1932676 個段落向量到索引...\n段落 FAISS 索引構建完成。索引中段落數: 1932676. 耗時: 5.63 秒\n正在將索引保存到: dpr_passage_faiss_index.idx\n索引保存成功。\n正在將新生成的段落映射保存到: dpr_passage_mapping.json\n段落映射保存成功。\n\n所有 DPR (段落級) 檢索組件準備就緒！\n\n--- 加載驗證數據 ---\n成功加載 1000 條記錄從 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n\n開始評估 DprPassageFaissRetriever (共 1000 個問題)...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating DprPassageFaissRetriever: 100%|██████████| 1000/1000 [34:04<00:00,  2.04s/it]","output_type":"stream"},{"name":"stdout","text":"\n\n--- DPR (段落級) 檢索器性能評估結果 (驗證集) ---\n------------------------------------------------------------\nRetriever                 | Recall@5        | MRR@5          \n------------------------------------------------------------\nDPR Passage               | 0.8050          | 0.6027         \n------------------------------------------------------------\n\n下一步：實現答案生成。\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:12:56.133932Z","iopub.execute_input":"2025-04-06T07:12:56.134363Z","iopub.status.idle":"2025-04-06T07:13:00.376337Z","shell.execute_reply.started":"2025-04-06T07:12:56.134329Z","shell.execute_reply":"2025-04-06T07:13:00.375337Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.11.0a2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.29.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nimport faiss\nimport numpy as np\nimport time\nimport os\nimport json\nfrom tqdm import tqdm\nimport re\nimport nltk\nfrom openai import OpenAI # *** 新增導入 ***\nimport sys # 用於退出\n\n# --- 配置 ---\nQUESTION_ENCODER_NAME = 'facebook/dpr-question_encoder-single-nq-base'\nCONTEXT_ENCODER_NAME = 'facebook/dpr-ctx_encoder-single-nq-base' # 僅在需要重建索引時加載\n# --- 修改：確保 DATA_DIR 路徑正確 ---\n# DATA_DIR = 'data' # 如果在本地運行\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data' # 根據你的環境調整\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl') # 使用驗證集生成預測\nTEST_FILE = os.path.join(DATA_DIR, 'test.jsonl') # 最終在測試集上運行\nPASSAGE_FAISS_INDEX_PATH = \"dpr_passage_faiss_index.idx\"\nPASSAGE_MAPPING_PATH = \"dpr_passage_mapping.json\"\n# --- 輸出文件配置 ---\nPREDICTION_TARGET = 'val' # 或 'test'\nOUTPUT_PREDICTION_FILE = f\"{PREDICTION_TARGET}_predict_dpr_passage.jsonl\"\n\n# --- 答案生成相關配置 ---\nNUM_CONTEXT_PASSAGES = 3\n# DPR 模型通常的最大序列長度\nMAX_LENGTH = 512 # 定義在這裡\nMAX_CONTEXT_LENGTH = 2000 # 根據 Qwen 模型限制調整\nLLM_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\" # 使用項目指定的模型\nLLM_MAX_TOKENS = 200 # LLM 生成答案的最大 token 數 (之前是 150，根據需要調整)\nLLM_TEMPERATURE = 0.7 # LLM 生成的溫度參數\n\n# --- SiliconFlow API 配置 (需要你填充！) ---\n# *** 重要：請將 YOUR_API_KEY 替換為你在 SiliconFlow 創建的真實 API 密鑰 ***\nSILICONFLOW_API_KEY = \"sk-xvrbrpercvlfxkfsxsfaidnwpvfjdwouqrxsauhxbdjnkmhh\" # 已填寫\nSILICONFLOW_BASE_URL = \"https://api.siliconflow.cn/v1\"\n\n# --- 設備檢測 ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用的設備: {device}\")\n\n# --- 初始化 SiliconFlow 客戶端 ---\nif SILICONFLOW_API_KEY == \"YOUR_API_KEY\":\n    print(\"警告：請在代碼中設置你的 SiliconFlow API 密鑰 (SILICONFLOW_API_KEY)。\")\n    llm_client = None\nelse:\n    try:\n        llm_client = OpenAI(api_key=SILICONFLOW_API_KEY, base_url=SILICONFLOW_BASE_URL)\n        print(\"SiliconFlow OpenAI 客戶端初始化成功。\")\n    except Exception as e:\n        print(f\"初始化 SiliconFlow OpenAI 客戶端時出錯: {e}\")\n        llm_client = None\n\n# --- 模型加載、數據加載、檢索器類定義 ---\ndef load_dpr_models(q_encoder_name, ctx_encoder_name, device):\n    # ... (加載邏輯，與之前相同) ...\n    print(f\"開始加載 DPR 模型...\")\n    start_time = time.time()\n    q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder = None, None, None, None\n    try:\n        print(f\"  加載問題分詞器: {q_encoder_name}\")\n        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(q_encoder_name)\n        print(f\"  加載問題編碼器: {q_encoder_name}\")\n        q_encoder = DPRQuestionEncoder.from_pretrained(q_encoder_name)\n        q_encoder.to(device).eval()\n        try:\n            print(f\"  嘗試加載上下文分詞器: {ctx_encoder_name}\")\n            ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)\n            print(f\"  嘗試加載上下文編碼器: {ctx_encoder_name}\")\n            ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)\n            ctx_encoder.to(device).eval()\n        except Exception as e_ctx:\n             print(f\"警告：加載上下文模型時出錯: {e_ctx}\")\n        end_time = time.time()\n        print(f\"DPR 模型加載完成（或部分完成）。耗時: {end_time - start_time:.2f} 秒\")\n        return q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder\n    except Exception as e_q:\n        print(f\"加載 DPR 問題模型時出錯，無法繼續: {e_q}\")\n        return None, None, None, None\n\n\ndef load_jsonl(file_path):\n    # ... (加載邏輯，與之前相同) ...\n    data = []\n    if not os.path.exists(file_path): return data\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip(): data.append(json.loads(line))\n    print(f\"成功加載 {len(data)} 條記錄從 {file_path}\")\n    return data\n\n# --- 修改：DprPassageFaissRetriever 類 ---\nclass DprPassageFaissRetriever:\n    def __init__(self, q_tokenizer, q_encoder, faiss_index, passage_mapping, device):\n        self.q_tokenizer = q_tokenizer\n        self.q_encoder = q_encoder\n        self.faiss_index = faiss_index\n        self.passage_mapping = passage_mapping\n        self.device = device\n        self.vector_dim = q_encoder.config.hidden_size if q_encoder else None\n        # *** 新增：將 MAX_LENGTH 存儲為實例屬性 ***\n        self.max_length = MAX_LENGTH\n        print(f\"DPR Passage Retriever 初始化完成。索引段落數: {self.faiss_index.ntotal if self.faiss_index else 'N/A'}\")\n\n    def retrieve_passages(self, query, top_k):\n        if not all([query, self.faiss_index, self.q_encoder, self.q_tokenizer, self.passage_mapping, self.vector_dim]): return []\n        try:\n            # *** 修改：使用 self.max_length ***\n            inputs = self.q_tokenizer(query, max_length=self.max_length, padding=False, truncation=True, return_tensors='pt')\n            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n            with torch.no_grad():\n                outputs = self.q_encoder(**inputs)\n                query_vector = outputs.pooler_output\n            norm = torch.linalg.norm(query_vector)\n            normalized_query_vector = query_vector / (norm + 1e-8)\n            query_np = normalized_query_vector.cpu().numpy().astype('float32').reshape(1, -1)\n            scores, indices = self.faiss_index.search(query_np, top_k)\n            results = []\n            valid_indices = indices[0]\n            valid_scores = scores[0]\n            for i, idx in enumerate(valid_indices):\n                if 0 <= idx < len(self.passage_mapping):\n                    original_doc_id, passage_text = self.passage_mapping[idx]\n                    results.append({\n                        \"doc_id\": original_doc_id,\n                        \"passage_text\": passage_text,\n                        \"score\": float(valid_scores[i]),\n                        \"passage_index_in_faiss\": int(idx)\n                    })\n            return results\n        except Exception as e: print(f\"檢索段落過程中出錯: {e}\"); return []\n\n    def retrieve(self, query, top_n=5):\n        top_k_passages = 20\n        passage_results = self.retrieve_passages(query, top_k=top_k_passages)\n        if not passage_results: return [-1]*top_n\n        doc_scores = {}\n        for p in passage_results:\n            doc_id = p[\"doc_id\"]\n            score = p[\"score\"]\n            if doc_id not in doc_scores or score > doc_scores[doc_id]: doc_scores[doc_id] = score\n        sorted_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n        top_n_doc_ids = [doc_id for doc_id, score in sorted_docs[:top_n]]\n        while len(top_n_doc_ids) < top_n:\n             top_n_doc_ids.append(-1)\n        return top_n_doc_ids[:top_n]\n\n\n# --- generate_answer_with_qwen 函數 (與之前相同) ---\ndef generate_answer_with_qwen(prompt):\n    \"\"\"\n    使用 SiliconFlow API (通過 OpenAI 兼容接口) 調用 Qwen LLM 生成答案。\n    \"\"\"\n    global llm_client\n    if llm_client is None:\n        print(\"錯誤：SiliconFlow 客戶端未初始化（可能缺少 API 密鑰）。\")\n        return \"LLM 客戶端未初始化\"\n    try:\n        response = llm_client.chat.completions.create(\n            model=LLM_MODEL_NAME,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=LLM_MAX_TOKENS,\n            temperature=LLM_TEMPERATURE,\n            stream=False\n        )\n        generated_answer = response.choices[0].message.content.strip()\n        return generated_answer\n    except Exception as e:\n        print(f\"調用 SiliconFlow API 時發生錯誤: {e}\")\n        return f\"調用 LLM 時出錯: {e}\"\n\n# --- build_prompt 函數 (與之前相同) ---\ndef build_prompt(question, context_passages):\n    \"\"\"\n    根據問題和檢索到的上下文段落構建 Prompt。\n    \"\"\"\n    if not context_passages:\n        context_str = \"沒有找到相關上下文。\"\n    else:\n        context_str = \"\\n\\n\".join(context_passages)\n        if len(context_str) > MAX_CONTEXT_LENGTH:\n            context_str = context_str[:MAX_CONTEXT_LENGTH] + \"...\"\n    prompt = f\"\"\"請根據以下提供的上下文信息來回答問題。如果上下文沒有提供足夠的信息，請回答 \"信息不足\"。\n\n上下文:\n{context_str}\n\n問題: {question}\n\n答案:\"\"\"\n    return prompt\n\n# --- 主程序：生成預測文件 (與之前相同) ---\nif __name__ == \"__main__\":\n    # 1. 加載 DPR 模型\n    question_tokenizer, question_encoder, _, _ = load_dpr_models(\n        QUESTION_ENCODER_NAME,\n        CONTEXT_ENCODER_NAME,\n        device\n    )\n\n    # 2. 加載 FAISS 索引和段落映射\n    passage_faiss_index = None\n    passage_mapping = []\n    # ... (省略索引和映射加載邏輯) ...\n    if os.path.exists(PASSAGE_FAISS_INDEX_PATH) and os.path.exists(PASSAGE_MAPPING_PATH):\n        try:\n            print(f\"\\n正在加載段落 FAISS 索引: {PASSAGE_FAISS_INDEX_PATH}\")\n            passage_faiss_index = faiss.read_index(PASSAGE_FAISS_INDEX_PATH)\n            print(f\"正在加載段落映射: {PASSAGE_MAPPING_PATH}\")\n            with open(PASSAGE_MAPPING_PATH, 'r') as f: passage_mapping = json.load(f)\n            if passage_faiss_index.ntotal != len(passage_mapping): raise ValueError(\"索引和映射數量不匹配！\")\n            print(\"索引和映射加載成功。\")\n        except Exception as e: print(f\"加載索引或映射失敗: {e}\"); passage_faiss_index = None\n    else: print(\"未找到索引或映射文件。\")\n\n\n    # 3. 檢查組件是否齊全\n    if all([passage_faiss_index, question_encoder, question_tokenizer, passage_mapping, llm_client]):\n        print(\"\\n所有檢索和 LLM 組件準備就緒！\")\n\n        # 4. 創建檢索器實例 (現在會存儲 max_length)\n        retriever = DprPassageFaissRetriever(\n            question_tokenizer,\n            question_encoder,\n            passage_faiss_index,\n            passage_mapping,\n            device\n        )\n\n        # 5. 加載目標數據集\n        target_file = VAL_FILE if PREDICTION_TARGET == 'val' else TEST_FILE\n        print(f\"\\n--- 加載目標數據集: {target_file} ---\")\n        target_data = load_jsonl(target_file)\n\n        if target_data:\n            print(f\"\\n--- 開始生成預測並寫入文件: {OUTPUT_PREDICTION_FILE} ---\")\n            # 6. 遍歷數據集，執行檢索、生成答案、寫入文件\n            with open(OUTPUT_PREDICTION_FILE, 'w', encoding='utf-8') as outfile:\n                for i, item in enumerate(tqdm(target_data, desc=f\"Generating {PREDICTION_TARGET} predictions\")):\n                    question = item['question']\n                    try:\n                        # a. 檢索上下文段落\n                        retrieved_passages_info = retriever.retrieve_passages(question, top_k=NUM_CONTEXT_PASSAGES)\n                        context_texts = [p['passage_text'] for p in retrieved_passages_info]\n                        # b. 檢索 Top-5 文檔 ID\n                        top_5_doc_ids = retriever.retrieve(question, top_n=5)\n                        # c. 構建 Prompt\n                        prompt = build_prompt(question, context_texts)\n                        # d. 調用 LLM 生成答案\n                        generated_answer = generate_answer_with_qwen(prompt)\n                        # e. 格式化輸出\n                        output_record = {\"question\": question, \"answer\": generated_answer, \"document_id\": top_5_doc_ids}\n                        # f. 寫入文件\n                        outfile.write(json.dumps(output_record, ensure_ascii=False) + '\\n')\n                    except Exception as e:\n                        print(f\"\\n處理問題 {i} ('{question}') 時發生錯誤: {e}\")\n                        error_record = {\"question\": question, \"answer\": f\"處理時發生錯誤: {e}\", \"document_id\": [-1]*5 }\n                        outfile.write(json.dumps(error_record, ensure_ascii=False) + '\\n')\n\n            print(f\"\\n預測文件 {OUTPUT_PREDICTION_FILE} 生成完成。\")\n            print(f\"下一步：使用 metrics_calculation.py 評估 {OUTPUT_PREDICTION_FILE} 文件 (如果目標是驗證集)。\")\n            if PREDICTION_TARGET == 'test':\n                 print(\"請將生成的 test_predict_dpr_passage.jsonl 文件用於最終提交。\")\n\n        else:\n            print(f\"未能加載目標數據集 {target_file}。\")\n\n    else:\n        # 提供更詳細的失敗原因\n        missing = []\n        if not passage_faiss_index: missing.append(\"FAISS 索引\")\n        if not question_encoder: missing.append(\"問題編碼器\")\n        if not question_tokenizer: missing.append(\"問題分詞器\")\n        if not passage_mapping: missing.append(\"段落映射\")\n        if not llm_client: missing.append(\"LLM 客戶端 (API Key?)\")\n        print(f\"\\n未能成功準備好用於生成預測的組件。缺少或失敗：{', '.join(missing)}。請檢查之前的錯誤信息。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:18:17.563029Z","iopub.execute_input":"2025-04-06T07:18:17.563531Z","iopub.status.idle":"2025-04-06T09:48:41.246102Z","shell.execute_reply.started":"2025-04-06T07:18:17.563494Z","shell.execute_reply":"2025-04-06T09:48:41.245041Z"}},"outputs":[{"name":"stdout","text":"使用的設備: cuda\nSiliconFlow OpenAI 客戶端初始化成功。\n開始加載 DPR 模型...\n  加載問題分詞器: facebook/dpr-question_encoder-single-nq-base\n  加載問題編碼器: facebook/dpr-question_encoder-single-nq-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"  嘗試加載上下文分詞器: facebook/dpr-ctx_encoder-single-nq-base\n  嘗試加載上下文編碼器: facebook/dpr-ctx_encoder-single-nq-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"DPR 模型加載完成（或部分完成）。耗時: 2.12 秒\n\n正在加載段落 FAISS 索引: dpr_passage_faiss_index.idx\n正在加載段落映射: dpr_passage_mapping.json\n索引和映射加載成功。\n\n所有檢索和 LLM 組件準備就緒！\nDPR Passage Retriever 初始化完成。索引段落數: 1932676\n\n--- 加載目標數據集: /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl ---\n成功加載 1000 條記錄從 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n\n--- 開始生成預測並寫入文件: val_predict_dpr_passage.jsonl ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions: 100%|██████████| 1000/1000 [2:30:07<00:00,  9.01s/it] ","output_type":"stream"},{"name":"stdout","text":"\n預測文件 val_predict_dpr_passage.jsonl 生成完成。\n下一步：使用 metrics_calculation.py 評估 val_predict_dpr_passage.jsonl 文件 (如果目標是驗證集)。\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"cd /kaggle/input/nq10k-comp5423/data_and_code","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:50:00.749209Z","iopub.execute_input":"2025-04-06T09:50:00.749522Z","iopub.status.idle":"2025-04-06T09:50:00.755493Z","shell.execute_reply.started":"2025-04-06T09:50:00.749497Z","shell.execute_reply":"2025-04-06T09:50:00.754827Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nq10k-comp5423/data_and_code\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!python /kaggle/input/nq10k-comp5423/data_and_code/metrics_calculation.py ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T10:23:45.128052Z","iopub.execute_input":"2025-04-06T10:23:45.128426Z","iopub.status.idle":"2025-04-06T10:23:45.732209Z","shell.execute_reply.started":"2025-04-06T10:23:45.128393Z","shell.execute_reply":"2025-04-06T10:23:45.731320Z"}},"outputs":[{"name":"stdout","text":"Evaluation Result:\nAnswer Accuracy:             0.0020\nDocument Retrieval Recall@5: 0.0010\nDocument Retrieval MRR@5   : 0.0003\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import torch\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nimport faiss\nimport numpy as np\nimport time\nimport os\nimport json\nfrom tqdm import tqdm\nimport re\nimport nltk\nfrom openai import OpenAI # *** 新增導入 ***\nimport sys # 用於退出\n\n# --- 配置 ---\nQUESTION_ENCODER_NAME = 'facebook/dpr-question_encoder-single-nq-base'\nCONTEXT_ENCODER_NAME = 'facebook/dpr-ctx_encoder-single-nq-base' # 僅在需要重建索引時加載\n# --- 修改：確保 DATA_DIR 路徑正確 ---\n# DATA_DIR = 'data' # 如果在本地運行\nDATA_DIR = '/kaggle/input/nq10k-comp5423/data_and_code/data' # 根據你的環境調整\nVAL_FILE = os.path.join(DATA_DIR, 'val.jsonl') # 使用驗證集生成預測\nTEST_FILE = os.path.join(DATA_DIR, 'test.jsonl') # 最終在測試集上運行\n\n# --- 修改：明確指定可寫目錄和文件路徑 ---\nWRITABLE_DIR = \"/kaggle/working/\" # Kaggle 的可寫目錄 (如果是其他環境，請修改)\n# 確保目錄存在 (雖然 /kaggle/working/ 通常存在)\nos.makedirs(WRITABLE_DIR, exist_ok=True)\n\nPASSAGE_FAISS_INDEX_PATH = os.path.join(WRITABLE_DIR, \"dpr_passage_faiss_index.idx\")\nPASSAGE_MAPPING_PATH = os.path.join(WRITABLE_DIR, \"dpr_passage_mapping.json\")\n# --- 輸出文件配置 ---\nPREDICTION_TARGET = 'val' # 或 'test'\n# *** 修改：使用 os.path.join 指定完整輸出路徑 ***\nOUTPUT_PREDICTION_FILE = os.path.join(WRITABLE_DIR, f\"{PREDICTION_TARGET}_predict_dpr_passage_debug.jsonl\") # 加個後綴區分\n\n# --- 答案生成相關配置 ---\nNUM_CONTEXT_PASSAGES = 3\nMAX_LENGTH = 512\nMAX_CONTEXT_LENGTH = 2000\nLLM_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\nLLM_MAX_TOKENS = 200\nLLM_TEMPERATURE = 0.7\n\n# --- SiliconFlow API 配置 ---\nSILICONFLOW_API_KEY = \"sk-xvrbrpercvlfxkfsxsfaidnwpvfjdwouqrxsauhxbdjnkmhh\" # 已填寫\nSILICONFLOW_BASE_URL = \"https://api.siliconflow.cn/v1\"\n\n# --- 設備檢測 ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用的設備: {device}\")\n\n# --- 初始化 SiliconFlow 客戶端 ---\nif SILICONFLOW_API_KEY == \"YOUR_API_KEY\" or not SILICONFLOW_API_KEY: # 檢查是否為空或佔位符\n    print(\"警告：請在代碼中設置你的 SiliconFlow API 密鑰 (SILICONFLOW_API_KEY)。\")\n    llm_client = None\nelse:\n    try:\n        llm_client = OpenAI(api_key=SILICONFLOW_API_KEY, base_url=SILICONFLOW_BASE_URL)\n        print(\"SiliconFlow OpenAI 客戶端初始化成功。\")\n    except Exception as e:\n        print(f\"初始化 SiliconFlow OpenAI 客戶端時出錯: {e}\")\n        llm_client = None\n\n# --- 模型加載、數據加載、檢索器類定義 ---\n# (與上一版本相同，確保 DprPassageFaissRetriever 類定義存在)\ndef load_dpr_models(q_encoder_name, ctx_encoder_name, device):\n    # ... (加載邏輯) ...\n    print(f\"開始加載 DPR 模型...\")\n    start_time = time.time()\n    q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder = None, None, None, None\n    try:\n        print(f\"  加載問題分詞器: {q_encoder_name}\")\n        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(q_encoder_name)\n        print(f\"  加載問題編碼器: {q_encoder_name}\")\n        q_encoder = DPRQuestionEncoder.from_pretrained(q_encoder_name)\n        q_encoder.to(device).eval()\n        try:\n            print(f\"  嘗試加載上下文分詞器: {ctx_encoder_name}\")\n            ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)\n            print(f\"  嘗試加載上下文編碼器: {ctx_encoder_name}\")\n            ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)\n            ctx_encoder.to(device).eval()\n        except Exception as e_ctx:\n             print(f\"警告：加載上下文模型時出錯: {e_ctx}\")\n        end_time = time.time()\n        print(f\"DPR 模型加載完成（或部分完成）。耗時: {end_time - start_time:.2f} 秒\")\n        return q_tokenizer, q_encoder, ctx_tokenizer, ctx_encoder\n    except Exception as e_q:\n        print(f\"加載 DPR 問題模型時出錯，無法繼續: {e_q}\")\n        return None, None, None, None\n\ndef load_jsonl(file_path):\n    # ... (加載邏輯) ...\n    data = []\n    if not os.path.exists(file_path): return data\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip(): data.append(json.loads(line))\n    print(f\"成功加載 {len(data)} 條記錄從 {file_path}\")\n    return data\n\nclass DprPassageFaissRetriever:\n    # ... (與上一版本相同) ...\n    def __init__(self, q_tokenizer, q_encoder, faiss_index, passage_mapping, device):\n        self.q_tokenizer = q_tokenizer\n        self.q_encoder = q_encoder\n        self.faiss_index = faiss_index\n        self.passage_mapping = passage_mapping\n        self.device = device\n        self.vector_dim = q_encoder.config.hidden_size if q_encoder else None\n        self.max_length = MAX_LENGTH\n        print(f\"DPR Passage Retriever 初始化完成。索引段落數: {self.faiss_index.ntotal if self.faiss_index else 'N/A'}\")\n\n    def retrieve_passages(self, query, top_k):\n        if not all([query, self.faiss_index, self.q_encoder, self.q_tokenizer, self.passage_mapping, self.vector_dim]): return []\n        try:\n            inputs = self.q_tokenizer(query, max_length=self.max_length, padding=False, truncation=True, return_tensors='pt')\n            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n            with torch.no_grad():\n                outputs = self.q_encoder(**inputs)\n                query_vector = outputs.pooler_output\n            norm = torch.linalg.norm(query_vector)\n            normalized_query_vector = query_vector / (norm + 1e-8)\n            query_np = normalized_query_vector.cpu().numpy().astype('float32').reshape(1, -1)\n            scores, indices = self.faiss_index.search(query_np, top_k)\n            results = []\n            valid_indices = indices[0]\n            valid_scores = scores[0]\n            for i, idx in enumerate(valid_indices):\n                if 0 <= idx < len(self.passage_mapping):\n                    original_doc_id, passage_text = self.passage_mapping[idx]\n                    results.append({\n                        \"doc_id\": original_doc_id,\n                        \"passage_text\": passage_text,\n                        \"score\": float(valid_scores[i]),\n                        \"passage_index_in_faiss\": int(idx)\n                    })\n            return results\n        except Exception as e: print(f\"檢索段落過程中出錯: {e}\"); return []\n\n    def retrieve(self, query, top_n=5):\n        top_k_passages = 20\n        passage_results = self.retrieve_passages(query, top_k=top_k_passages)\n        if not passage_results: return [-1]*top_n\n        doc_scores = {}\n        for p in passage_results:\n            doc_id = p[\"doc_id\"]\n            score = p[\"score\"]\n            # 確保 doc_id 是有效的整數或可比較類型\n            if not isinstance(doc_id, (int, np.integer)):\n                 print(f\"警告：在聚合時遇到非整數 doc_id: {doc_id} (類型: {type(doc_id)})\")\n                 continue # 跳過無效的 doc_id\n            if doc_id not in doc_scores or score > doc_scores[doc_id]: doc_scores[doc_id] = score\n        sorted_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n        top_n_doc_ids = [doc_id for doc_id, score in sorted_docs[:top_n]]\n        while len(top_n_doc_ids) < top_n:\n             top_n_doc_ids.append(-1) # 使用 -1 作為填充/錯誤標識\n        # 確保返回列表中的元素都是整數\n        final_ids = []\n        for doc_id in top_n_doc_ids[:top_n]:\n            try:\n                final_ids.append(int(doc_id))\n            except (ValueError, TypeError):\n                print(f\"警告：無法將 doc_id '{doc_id}' 轉換為整數，使用 -1 替代。\")\n                final_ids.append(-1) # 如果轉換失敗，也使用 -1\n        return final_ids\n\n\n# --- generate_answer_with_qwen 函數 (與之前相同) ---\ndef generate_answer_with_qwen(prompt):\n    \"\"\"\n    使用 SiliconFlow API (通過 OpenAI 兼容接口) 調用 Qwen LLM 生成答案。\n    \"\"\"\n    global llm_client\n    if llm_client is None:\n        print(\"錯誤：SiliconFlow 客戶端未初始化（可能缺少 API 密鑰）。\")\n        return \"LLM 客戶端未初始化\"\n    try:\n        response = llm_client.chat.completions.create(\n            model=LLM_MODEL_NAME,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=LLM_MAX_TOKENS,\n            temperature=LLM_TEMPERATURE,\n            stream=False\n        )\n        generated_answer = response.choices[0].message.content.strip()\n        # 基本的答案清理：移除可能的引號\n        if generated_answer.startswith('\"') and generated_answer.endswith('\"'):\n             generated_answer = generated_answer[1:-1]\n        if generated_answer.startswith(\"'\") and generated_answer.endswith(\"'\"):\n             generated_answer = generated_answer[1:-1]\n        # 移除常見的拒絕回答或前綴\n        refusal_prefixes = [\"信息不足。\", \"根據提供的上下文信息，\", \"根據上下文信息，\", \"答案:\"]\n        for prefix in refusal_prefixes:\n             if generated_answer.startswith(prefix):\n                  generated_answer = generated_answer[len(prefix):].strip()\n\n        return generated_answer if generated_answer else \"未能生成答案\" # 避免返回空字符串\n\n    except Exception as e:\n        print(f\"調用 SiliconFlow API 時發生錯誤: {type(e).__name__} - {e}\")\n        return f\"調用 LLM 時出錯\" # 返回錯誤信息\n\n# --- build_prompt 函數 (與之前相同) ---\ndef build_prompt(question, context_passages):\n    \"\"\"\n    根據問題和檢索到的上下文段落構建 Prompt。\n    \"\"\"\n    if not context_passages:\n        context_str = \"沒有找到相關上下文。\"\n    else:\n        context_str = \"\\n\\n\".join(context_passages)\n        if len(context_str) > MAX_CONTEXT_LENGTH:\n            context_str = context_str[:MAX_CONTEXT_LENGTH] + \"...\"\n    prompt = f\"\"\"請根據以下提供的上下文信息來回答問題。如果上下文沒有提供足夠的信息，請回答 \"信息不足\"。\n\n上下文:\n{context_str}\n\n問題: {question}\n\n答案:\"\"\"\n    return prompt\n\n# --- 主程序：生成預測文件 ---\nif __name__ == \"__main__\":\n    # 1. 加載 DPR 模型\n    question_tokenizer, question_encoder, _, _ = load_dpr_models(\n        QUESTION_ENCODER_NAME,\n        CONTEXT_ENCODER_NAME,\n        device\n    )\n\n    # 2. 加載 FAISS 索引和段落映射\n    passage_faiss_index = None\n    passage_mapping = []\n    # ... (省略索引和映射加載邏輯) ...\n    if os.path.exists(PASSAGE_FAISS_INDEX_PATH) and os.path.exists(PASSAGE_MAPPING_PATH):\n        try:\n            print(f\"\\n正在加載段落 FAISS 索引: {PASSAGE_FAISS_INDEX_PATH}\")\n            passage_faiss_index = faiss.read_index(PASSAGE_FAISS_INDEX_PATH)\n            print(f\"正在加載段落映射: {PASSAGE_MAPPING_PATH}\")\n            with open(PASSAGE_MAPPING_PATH, 'r') as f: passage_mapping = json.load(f)\n            if passage_faiss_index.ntotal != len(passage_mapping): raise ValueError(\"索引和映射數量不匹配！\")\n            print(\"索引和映射加載成功。\")\n        except Exception as e: print(f\"加載索引或映射失敗: {e}\"); passage_faiss_index = None\n    else: print(f\"未找到索引或映射文件 ({PASSAGE_FAISS_INDEX_PATH}, {PASSAGE_MAPPING_PATH})。需要先運行索引構建步驟。\")\n\n\n    # 3. 檢查組件是否齊全\n    if all([passage_faiss_index, question_encoder, question_tokenizer, passage_mapping, llm_client]):\n        print(\"\\n所有檢索和 LLM 組件準備就緒！\")\n\n        # 4. 創建檢索器實例\n        retriever = DprPassageFaissRetriever(\n            question_tokenizer,\n            question_encoder,\n            passage_faiss_index,\n            passage_mapping,\n            device\n        )\n\n        # 5. 加載目標數據集\n        target_file = VAL_FILE if PREDICTION_TARGET == 'val' else TEST_FILE\n        print(f\"\\n--- 加載目標數據集: {target_file} ---\")\n        target_data = load_jsonl(target_file)\n\n        if target_data:\n            print(f\"\\n--- 開始生成預測並寫入文件: {OUTPUT_PREDICTION_FILE} ---\")\n            # 6. 遍歷數據集，執行檢索、生成答案、寫入文件\n            with open(OUTPUT_PREDICTION_FILE, 'w', encoding='utf-8') as outfile:\n                for i, item in enumerate(tqdm(target_data, desc=f\"Generating {PREDICTION_TARGET} predictions\")):\n                    question = item['question']\n                    generated_answer = \"處理時發生內部錯誤\" # 默認錯誤答案\n                    top_5_doc_ids = [-1]*5 # 默認錯誤ID\n\n                    try:\n                        # a. 檢索上下文段落\n                        retrieved_passages_info = retriever.retrieve_passages(question, top_k=NUM_CONTEXT_PASSAGES)\n                        context_texts = [p['passage_text'] for p in retrieved_passages_info]\n\n                        # b. 檢索 Top-5 文檔 ID\n                        top_5_doc_ids = retriever.retrieve(question, top_n=5) # 確保返回的是 list[int]\n\n                        # c. 構建 Prompt\n                        prompt = build_prompt(question, context_texts)\n\n                        # d. 調用 LLM 生成答案\n                        generated_answer = generate_answer_with_qwen(prompt)\n\n                        # e. 格式化輸出\n                        output_record = {\n                            \"question\": question,\n                            \"answer\": str(generated_answer), # 確保答案是字符串\n                            \"document_id\": top_5_doc_ids # 確保這是整數列表\n                        }\n\n                        # *** 加入詳細的 Debug 打印 ***\n                        print(f\"\\n--- DEBUG: Preparing to write record {i} ---\")\n                        print(f\"  Question Type: {type(question)}, Value: '{question[:100]}...'\") # 打印部分問題\n                        print(f\"  Answer Type: {type(generated_answer)}, Value: '{generated_answer}'\")\n                        # 檢查 generated_answer 是否包含錯誤信息\n                        if isinstance(generated_answer, str) and generated_answer.startswith(\"調用 LLM 時出錯\"):\n                            print(\"  WARNING: Generated answer indicates LLM call error.\")\n                        print(f\"  Doc IDs Type: {type(top_5_doc_ids)}, Value: {top_5_doc_ids}\")\n                        # 檢查 Doc IDs 列表內容\n                        if isinstance(top_5_doc_ids, list):\n                            print(f\"  Doc IDs List Length: {len(top_5_doc_ids)}\")\n                            if top_5_doc_ids and len(top_5_doc_ids) > 0 and top_5_doc_ids[0] != -1 : # 檢查第一個有效ID的類型\n                                print(f\"  First Valid Doc ID Type: {type(top_5_doc_ids[0])}\")\n                        else:\n                             print(f\"  WARNING: Doc IDs is not a list!\")\n                        print(f\"  Output Record Dict: {output_record}\")\n                        print(f\"--- End Debug Info ---\")\n                        # *** Debug 打印結束 ***\n\n                        # f. 寫入文件\n                        outfile.write(json.dumps(output_record, ensure_ascii=False) + '\\n')\n\n                    except Exception as e:\n                        print(f\"\\n處理問題 {i} ('{question}') 時發生嚴重錯誤: {e}\")\n                        # 寫入包含錯誤信息的記錄\n                        error_record = {\n                            \"question\": question,\n                            \"answer\": f\"處理時發生嚴重錯誤: {e}\",\n                            \"document_id\": [-1]*5 # 使用錯誤標識符\n                        }\n                        # 在寫入錯誤記錄前也打印一下，以防萬一\n                        print(f\"\\n--- DEBUG: Writing ERROR record {i} ---\")\n                        print(f\"  Error Record Dict: {error_record}\")\n                        print(f\"--- End Debug Info ---\")\n                        outfile.write(json.dumps(error_record, ensure_ascii=False) + '\\n')\n\n\n            print(f\"\\n預測文件 {OUTPUT_PREDICTION_FILE} 生成完成。\")\n            print(f\"下一步：使用 metrics_calculation.py 評估 {OUTPUT_PREDICTION_FILE} 文件 (如果目標是驗證集)。\")\n            if PREDICTION_TARGET == 'test':\n                 print(\"請將生成的 test_predict_dpr_passage_debug.jsonl 文件用於最終提交。\")\n\n        else:\n            print(f\"未能加載目標數據集 {target_file}。\")\n\n    else:\n        # 提供更詳細的失敗原因\n        missing = []\n        if not passage_faiss_index: missing.append(\"FAISS 索引\")\n        if not question_encoder: missing.append(\"問題編碼器\")\n        if not question_tokenizer: missing.append(\"問題分詞器\")\n        if not passage_mapping: missing.append(\"段落映射\")\n        if not llm_client: missing.append(\"LLM 客戶端 (API Key?)\")\n        print(f\"\\n未能成功準備好用於生成預測的組件。缺少或失敗：{', '.join(missing)}。請檢查之前的錯誤信息。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T10:14:17.572785Z","iopub.execute_input":"2025-04-06T10:14:17.573141Z","iopub.status.idle":"2025-04-06T10:19:27.991234Z","shell.execute_reply.started":"2025-04-06T10:14:17.573114Z","shell.execute_reply":"2025-04-06T10:19:27.989353Z"}},"outputs":[{"name":"stdout","text":"使用的設備: cuda\nSiliconFlow OpenAI 客戶端初始化成功。\n開始加載 DPR 模型...\n  加載問題分詞器: facebook/dpr-question_encoder-single-nq-base\n  加載問題編碼器: facebook/dpr-question_encoder-single-nq-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"  嘗試加載上下文分詞器: facebook/dpr-ctx_encoder-single-nq-base\n  嘗試加載上下文編碼器: facebook/dpr-ctx_encoder-single-nq-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"DPR 模型加載完成（或部分完成）。耗時: 2.29 秒\n\n正在加載段落 FAISS 索引: /kaggle/working/dpr_passage_faiss_index.idx\n正在加載段落映射: /kaggle/working/dpr_passage_mapping.json\n索引和映射加載成功。\n\n所有檢索和 LLM 組件準備就緒！\nDPR Passage Retriever 初始化完成。索引段落數: 1932676\n\n--- 加載目標數據集: /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl ---\n成功加載 1000 條記錄從 /kaggle/input/nq10k-comp5423/data_and_code/data/val.jsonl\n\n--- 開始生成預測並寫入文件: /kaggle/working/val_predict_dpr_passage_debug.jsonl ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   0%|          | 1/1000 [00:09<2:39:23,  9.57s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 0 ---\n  Question Type: <class 'str'>, Value: 'when did the british first land in north america...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，英国首先在北美登陆的时间是1607年。具体信息来自文本中的这句话：“The first successful English settlement was established in 1607.” 这里的“settlement”可以理解为登陆并建立定居点。'\n  Doc IDs Type: <class 'list'>, Value: [8502, 5655, 4507, 5318, 5222]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'when did the british first land in north america', 'answer': '根据提供的上下文信息，英国首先在北美登陆的时间是1607年。具体信息来自文本中的这句话：“The first successful English settlement was established in 1607.” 这里的“settlement”可以理解为登陆并建立定居点。', 'document_id': [8502, 5655, 4507, 5318, 5222]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   0%|          | 2/1000 [00:17<2:25:45,  8.76s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 1 ---\n  Question Type: <class 'str'>, Value: 'when did the 1st world war officially end...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，第一次世界大战于1918年11月11日正式结束。具体答案如下：\n\n答案: 1918年11月11日'\n  Doc IDs Type: <class 'list'>, Value: [10722, 2266, 10128, 3325, 2011]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'when did the 1st world war officially end', 'answer': '根据提供的上下文信息，第一次世界大战于1918年11月11日正式结束。具体答案如下：\\n\\n答案: 1918年11月11日', 'document_id': [10722, 2266, 10128, 3325, 2011]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   0%|          | 3/1000 [00:25<2:17:29,  8.27s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 2 ---\n  Question Type: <class 'str'>, Value: 'who's the girl that plays the new wonder woman...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，扮演新神奇女侠的是Gal Gadot。'\n  Doc IDs Type: <class 'list'>, Value: [7556, 5970, 8669, 10786, 8552]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': \"who's the girl that plays the new wonder woman\", 'answer': '根据提供的上下文信息，扮演新神奇女侠的是Gal Gadot。', 'document_id': [7556, 5970, 8669, 10786, 8552]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   0%|          | 4/1000 [00:34<2:23:49,  8.66s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 3 ---\n  Question Type: <class 'str'>, Value: 'who is the director of the cia today...'\n  Answer Type: <class 'str'>, Value: 'Context中提到的是美国国家情报总监（Director of National Intelligence）的现任负责人是Christopher A. Wray，但并未提及中央情报局局长（Director of CIA）。根据提供的信息，无法确定中央情报局局长的现任负责人。'\n  Doc IDs Type: <class 'list'>, Value: [457, 3556, 4733, 11721, 7816]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who is the director of the cia today', 'answer': 'Context中提到的是美国国家情报总监（Director of National Intelligence）的现任负责人是Christopher A. Wray，但并未提及中央情报局局长（Director of CIA）。根据提供的信息，无法确定中央情报局局长的现任负责人。', 'document_id': [457, 3556, 4733, 11721, 7816]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   0%|          | 5/1000 [00:42<2:18:00,  8.32s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 4 ---\n  Question Type: <class 'str'>, Value: 'who plays ben in the new fantastic four...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，Ben Grimm / The Thing 由 Toby Kebbell 扮演。'\n  Doc IDs Type: <class 'list'>, Value: [8635, 11119, 1784, 11160, 9192]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who plays ben in the new fantastic four', 'answer': '根据提供的上下文信息，Ben Grimm / The Thing 由 Toby Kebbell 扮演。', 'document_id': [8635, 11119, 1784, 11160, 9192]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|          | 6/1000 [00:51<2:20:42,  8.49s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 5 ---\n  Question Type: <class 'str'>, Value: 'when is the men's ice hockey winter olympics 2018...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，2018年冬季奥运会男子冰球比赛在2018年2月14日至25日在韩国江陵举行。\n\n答案: 2018年2月14日至25日'\n  Doc IDs Type: <class 'list'>, Value: [10903, 8675, 4047, 1725, 6126]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': \"when is the men's ice hockey winter olympics 2018\", 'answer': '根据提供的上下文信息，2018年冬季奥运会男子冰球比赛在2018年2月14日至25日在韩国江陵举行。\\n\\n答案: 2018年2月14日至25日', 'document_id': [10903, 8675, 4047, 1725, 6126]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|          | 7/1000 [00:58<2:15:15,  8.17s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 6 ---\n  Question Type: <class 'str'>, Value: 'who plays fiddle on don't pass me by...'\n  Answer Type: <class 'str'>, Value: '信息不足'\n  Doc IDs Type: <class 'list'>, Value: [5401, 11066, 1479, 8266, 10088]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': \"who plays fiddle on don't pass me by\", 'answer': '信息不足', 'document_id': [5401, 11066, 1479, 8266, 10088]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|          | 8/1000 [01:06<2:12:56,  8.04s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 7 ---\n  Question Type: <class 'str'>, Value: 'who played drusilla on the young and the restless...'\n  Answer Type: <class 'str'>, Value: 'Rowell played Drucilla on The Young and the Restless。'\n  Doc IDs Type: <class 'list'>, Value: [11702, 501, 7345, 7087, 5144]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who played drusilla on the young and the restless', 'answer': 'Rowell played Drucilla on The Young and the Restless。', 'document_id': [11702, 501, 7345, 7087, 5144]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|          | 9/1000 [01:14<2:11:57,  7.99s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 8 ---\n  Question Type: <class 'str'>, Value: 'name of the actor who plays captain america...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，扮演美国队长（Captain America）的演员是克里斯·埃文斯（Chris Evans）。'\n  Doc IDs Type: <class 'list'>, Value: [12061, 11050, 5426, 8347, 3035]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'name of the actor who plays captain america', 'answer': '根据提供的上下文信息，扮演美国队长（Captain America）的演员是克里斯·埃文斯（Chris Evans）。', 'document_id': [12061, 11050, 5426, 8347, 3035]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|          | 10/1000 [01:21<2:09:46,  7.86s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 9 ---\n  Question Type: <class 'str'>, Value: 'where is the national multiple sclerosis society located...'\n  Answer Type: <class 'str'>, Value: 'The National Multiple Sclerosis Society (NMSS) is based in New York City.'\n  Doc IDs Type: <class 'list'>, Value: [6708, 145, 10932, 7939, 9558]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'where is the national multiple sclerosis society located', 'answer': 'The National Multiple Sclerosis Society (NMSS) is based in New York City.', 'document_id': [6708, 145, 10932, 7939, 9558]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|          | 11/1000 [01:29<2:07:11,  7.72s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 10 ---\n  Question Type: <class 'str'>, Value: 'who is financing the curse of oak island...'\n  Answer Type: <class 'str'>, Value: '信息不足'\n  Doc IDs Type: <class 'list'>, Value: [8103, 6194, 10946, 3885, 7376]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who is financing the curse of oak island', 'answer': '信息不足', 'document_id': [8103, 6194, 10946, 3885, 7376]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|          | 12/1000 [01:37<2:09:21,  7.86s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 11 ---\n  Question Type: <class 'str'>, Value: 'where is the most gold stored in the world...'\n  Answer Type: <class 'str'>, Value: '提供的上下文信息没有提及世界上黄金存储的具体地点。'\n  Doc IDs Type: <class 'list'>, Value: [7338, 9823, 9187, 3616, 10102]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'where is the most gold stored in the world', 'answer': '提供的上下文信息没有提及世界上黄金存储的具体地点。', 'document_id': [7338, 9823, 9187, 3616, 10102]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|▏         | 13/1000 [01:47<2:21:23,  8.60s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 12 ---\n  Question Type: <class 'str'>, Value: 'what age can you get a tattoo in louisiana...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，所有50个州和哥伦比亚特区都有法律规定，接受纹身的人必须年满18岁。因此，在路易斯安那州，你可以年满18岁时进行纹身。答案是18岁。'\n  Doc IDs Type: <class 'list'>, Value: [10489, 7176, 1857, 2499, 7120]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'what age can you get a tattoo in louisiana', 'answer': '根据提供的上下文信息，所有50个州和哥伦比亚特区都有法律规定，接受纹身的人必须年满18岁。因此，在路易斯安那州，你可以年满18岁时进行纹身。答案是18岁。', 'document_id': [10489, 7176, 1857, 2499, 7120]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   1%|▏         | 14/1000 [01:56<2:19:32,  8.49s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 13 ---\n  Question Type: <class 'str'>, Value: 'who plays the cop in once upon a time...'\n  Answer Type: <class 'str'>, Value: 'James Dornan plays the cop in Once Upon a Time. Specifically, he played Sheriff Graham Humbert in the ABC series Once Upon a Time (2011 -- 2013).'\n  Doc IDs Type: <class 'list'>, Value: [10373, 5461, 8352, 4888, 5234]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who plays the cop in once upon a time', 'answer': 'James Dornan plays the cop in Once Upon a Time. Specifically, he played Sheriff Graham Humbert in the ABC series Once Upon a Time (2011 -- 2013).', 'document_id': [10373, 5461, 8352, 4888, 5234]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 15/1000 [02:04<2:16:36,  8.32s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 14 ---\n  Question Type: <class 'str'>, Value: 'who played young martha may in the grinch...'\n  Answer Type: <class 'str'>, Value: '信息不足'\n  Doc IDs Type: <class 'list'>, Value: [8260, 6768, 3309, 1107, 1351]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who played young martha may in the grinch', 'answer': '信息不足', 'document_id': [8260, 6768, 3309, 1107, 1351]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 16/1000 [02:12<2:19:35,  8.51s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 15 ---\n  Question Type: <class 'str'>, Value: 'when did the first harry potter book get released...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，第一本哈利·波特书籍《哈利·波特与魔法石》（Harry Potter and the Philosopher's Stone）是在1997年6月26日发布的。'\n  Doc IDs Type: <class 'list'>, Value: [12044, 2256, 284, 5621, 7470]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'when did the first harry potter book get released', 'answer': \"根据提供的上下文信息，第一本哈利·波特书籍《哈利·波特与魔法石》（Harry Potter and the Philosopher's Stone）是在1997年6月26日发布的。\", 'document_id': [12044, 2256, 284, 5621, 7470]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 17/1000 [02:22<2:21:58,  8.67s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 16 ---\n  Question Type: <class 'str'>, Value: 'when did mexico come under spanish colonial rule...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文，墨西哥被西班牙殖民统治始于1519年， Hernán Cortés 和他的同伴开始他们的探索和征服之旅。具体来说，Cortés 和他的队伍是在1519年2月登陆墨西哥的。因此，墨西哥正式被西班牙殖民统治的时间是1519年。'\n  Doc IDs Type: <class 'list'>, Value: [7128, 5522, 8953, 10252, 10690]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'when did mexico come under spanish colonial rule', 'answer': '根据提供的上下文，墨西哥被西班牙殖民统治始于1519年， Hernán Cortés 和他的同伴开始他们的探索和征服之旅。具体来说，Cortés 和他的队伍是在1519年2月登陆墨西哥的。因此，墨西哥正式被西班牙殖民统治的时间是1519年。', 'document_id': [7128, 5522, 8953, 10252, 10690]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 18/1000 [02:29<2:17:49,  8.42s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 17 ---\n  Question Type: <class 'str'>, Value: 'how many seats in the house and senate...'\n  Answer Type: <class 'str'>, Value: '众议院有435个席位，参议院有100个席位。'\n  Doc IDs Type: <class 'list'>, Value: [7224, 801, 4798, 10243, 10156]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'how many seats in the house and senate', 'answer': '众议院有435个席位，参议院有100个席位。', 'document_id': [7224, 801, 4798, 10243, 10156]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 19/1000 [02:37<2:14:44,  8.24s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 18 ---\n  Question Type: <class 'str'>, Value: 'who plays dr jo wilson on grey's anatomy...'\n  Answer Type: <class 'str'>, Value: 'Camilla Luddington plays Dr. Jo Wilson on Grey's Anatomy.'\n  Doc IDs Type: <class 'list'>, Value: [8688, 5542, 10256, 6931, 9314]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': \"who plays dr jo wilson on grey's anatomy\", 'answer': \"Camilla Luddington plays Dr. Jo Wilson on Grey's Anatomy.\", 'document_id': [8688, 5542, 10256, 6931, 9314]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 20/1000 [02:46<2:17:38,  8.43s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 19 ---\n  Question Type: <class 'str'>, Value: 'who sang the song please come to boston...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，歌曲 \"Please Come to Boston\" 是由 American singer-songwriter Dave Loggins 撰写并录制的。所以答案是 Dave Loggins。'\n  Doc IDs Type: <class 'list'>, Value: [10214, 11512, 11066, 6936, 8732]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who sang the song please come to boston', 'answer': '根据提供的上下文信息，歌曲 \"Please Come to Boston\" 是由 American singer-songwriter Dave Loggins 撰写并录制的。所以答案是 Dave Loggins。', 'document_id': [10214, 11512, 11066, 6936, 8732]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 21/1000 [02:54<2:17:28,  8.43s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 20 ---\n  Question Type: <class 'str'>, Value: 'when does the new season of the arrangement come out...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，第二季于2017年4月13日宣布续订，但具体播出日期没有提供。因此，答案是：信息不足。'\n  Doc IDs Type: <class 'list'>, Value: [8741, 7444, 7064, 6264, 420]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'when does the new season of the arrangement come out', 'answer': '根据提供的上下文信息，第二季于2017年4月13日宣布续订，但具体播出日期没有提供。因此，答案是：信息不足。', 'document_id': [8741, 7444, 7064, 6264, 420]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 22/1000 [03:02<2:13:24,  8.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 21 ---\n  Question Type: <class 'str'>, Value: 'the point at which a planet is the greatest distance away from the sun...'\n  Answer Type: <class 'str'>, Value: '蚀日（aphelion）'\n  Doc IDs Type: <class 'list'>, Value: [7744, 1623, 4672, 489, 5523]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'the point at which a planet is the greatest distance away from the sun', 'answer': '蚀日（aphelion）', 'document_id': [7744, 1623, 4672, 489, 5523]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 23/1000 [03:10<2:14:08,  8.24s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 22 ---\n  Question Type: <class 'str'>, Value: 'who is the vice president if the president dies...'\n  Answer Type: <class 'str'>, Value: '如果总统去世，副总统将接任总统职务。'\n  Doc IDs Type: <class 'list'>, Value: [10656, 11725, 563, 3975, 3222]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who is the vice president if the president dies', 'answer': '如果总统去世，副总统将接任总统职务。', 'document_id': [10656, 11725, 563, 3975, 3222]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▏         | 24/1000 [03:21<2:23:23,  8.81s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 23 ---\n  Question Type: <class 'str'>, Value: 'when do purple martins migrate to south america...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，紫 Martins 在春季从南美洲迁移到北美繁殖，因此它们在冬季在南美洲度过。具体来说，文中有提到 \"Wintering in South America, purple martins migrate to North America in spring to breed.\" 因此，紫 Martins 迁移到南美洲的时间是冬季。'\n  Doc IDs Type: <class 'list'>, Value: [4651, 4057, 8650, -1, -1]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'when do purple martins migrate to south america', 'answer': '根据提供的上下文信息，紫 Martins 在春季从南美洲迁移到北美繁殖，因此它们在冬季在南美洲度过。具体来说，文中有提到 \"Wintering in South America, purple martins migrate to North America in spring to breed.\" 因此，紫 Martins 迁移到南美洲的时间是冬季。', 'document_id': [4651, 4057, 8650, -1, -1]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   2%|▎         | 25/1000 [03:30<2:25:14,  8.94s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 24 ---\n  Question Type: <class 'str'>, Value: 'loss of memory due to the passage of time during which the memory trace is not used...'\n  Answer Type: <class 'str'>, Value: '信息不足\n\n根据提供的上下文信息，没有直接提到由于时间流逝导致记忆痕迹未被使用从而造成的记忆丧失。提供的信息主要讨论了时间片错误、干扰理论以及注意力缺失等问题，但没有具体提到因时间流逝而导致的记忆丧失情况。'\n  Doc IDs Type: <class 'list'>, Value: [6319, 3775, 3707, 5265, 10285]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'loss of memory due to the passage of time during which the memory trace is not used', 'answer': '信息不足\\n\\n根据提供的上下文信息，没有直接提到由于时间流逝导致记忆痕迹未被使用从而造成的记忆丧失。提供的信息主要讨论了时间片错误、干扰理论以及注意力缺失等问题，但没有具体提到因时间流逝而导致的记忆丧失情况。', 'document_id': [6319, 3775, 3707, 5265, 10285]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 26/1000 [03:41<2:35:52,  9.60s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 25 ---\n  Question Type: <class 'str'>, Value: 'the printmaking technique that uses acid to cut the lines of the image is called...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，使用酸来切割图像线条的版画技术是蚀刻（etching）。\n\n蚀刻是一种版画技术，在这种技术中，金属（通常为铜、锌或钢）制成的平板覆盖着一层蜡质或丙烯酸地膜。艺术家然后使用尖锐的蚀刻针穿过地膜进行绘制。裸露的金属线条随后浸入酸性溶液（例如硝酸或氯化铁）中，酸溶液会蚀刻裸露的金属，留下线条痕迹。因此，答案是蚀刻。'\n  Doc IDs Type: <class 'list'>, Value: [12001, 687, 1102, 5113, -1]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'the printmaking technique that uses acid to cut the lines of the image is called', 'answer': '根据提供的上下文信息，使用酸来切割图像线条的版画技术是蚀刻（etching）。\\n\\n蚀刻是一种版画技术，在这种技术中，金属（通常为铜、锌或钢）制成的平板覆盖着一层蜡质或丙烯酸地膜。艺术家然后使用尖锐的蚀刻针穿过地膜进行绘制。裸露的金属线条随后浸入酸性溶液（例如硝酸或氯化铁）中，酸溶液会蚀刻裸露的金属，留下线条痕迹。因此，答案是蚀刻。', 'document_id': [12001, 687, 1102, 5113, -1]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 27/1000 [03:50<2:35:14,  9.57s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 26 ---\n  Question Type: <class 'str'>, Value: 'who was the original actor in walking tall...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，乔·唐·贝克（Joe Don Baker）是1973年电影《Walking Tall》的主演。因此，乔·唐·贝克是《Walking Tall》的原始演员。'\n  Doc IDs Type: <class 'list'>, Value: [740, 321, 478, 4932, 5106]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who was the original actor in walking tall', 'answer': '根据提供的上下文信息，乔·唐·贝克（Joe Don Baker）是1973年电影《Walking Tall》的主演。因此，乔·唐·贝克是《Walking Tall》的原始演员。', 'document_id': [740, 321, 478, 4932, 5106]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 28/1000 [03:59<2:27:37,  9.11s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 27 ---\n  Question Type: <class 'str'>, Value: 'who plays jamie in a walk to remember...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，没有提到任何扮演Jamie这个角色的演员。'\n  Doc IDs Type: <class 'list'>, Value: [7538, 1916, 4306, 2460, 3394]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who plays jamie in a walk to remember', 'answer': '根据提供的上下文信息，没有提到任何扮演Jamie这个角色的演员。', 'document_id': [7538, 1916, 4306, 2460, 3394]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 29/1000 [04:06<2:19:01,  8.59s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 28 ---\n  Question Type: <class 'str'>, Value: 'what channel is fox sports on dish network...'\n  Answer Type: <class 'str'>, Value: '信息不足'\n  Doc IDs Type: <class 'list'>, Value: [8000, 4775, 9089, 4489, 11095]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'what channel is fox sports on dish network', 'answer': '信息不足', 'document_id': [8000, 4775, 9089, 4489, 11095]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 30/1000 [04:13<2:12:54,  8.22s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 29 ---\n  Question Type: <class 'str'>, Value: 'who won the women's college basketball championship 2018...'\n  Answer Type: <class 'str'>, Value: '信息不足'\n  Doc IDs Type: <class 'list'>, Value: [10444, 12018, 8239, 9500, 11719]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': \"who won the women's college basketball championship 2018\", 'answer': '信息不足', 'document_id': [10444, 12018, 8239, 9500, 11719]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 31/1000 [04:23<2:19:15,  8.62s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 30 ---\n  Question Type: <class 'str'>, Value: 'who played the district attorney in law and order...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，没有明确提到谁扮演了检察官（District Attorney）的角色。上下文提到了Henry Sharpe，他是导演，但并未说明他是否扮演了检察官。因此，答案是：\n\n信息不足'\n  Doc IDs Type: <class 'list'>, Value: [7847, 9412, 8291, 948, 3327]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'who played the district attorney in law and order', 'answer': '根据提供的上下文信息，没有明确提到谁扮演了检察官（District Attorney）的角色。上下文提到了Henry Sharpe，他是导演，但并未说明他是否扮演了检察官。因此，答案是：\\n\\n信息不足', 'document_id': [7847, 9412, 8291, 948, 3327]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 32/1000 [04:38<2:52:45, 10.71s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 31 ---\n  Question Type: <class 'str'>, Value: 'where is the gizzard located on the chicken...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，关于鸡的胃囊的具体位置描述不够明确。但是，可以知道鸡的胃囊是消化道的一部分，并且在“glandular stomach (腺胃) ”之后，“Then the food passes into the gizzard ( also known as the muscular stomach or ventriculus ) .” 这句话说明食物进入胃囊。结合“Gizzard of a chicken <P> The gizzard, also referred to as the ventriculus, gastric mill, and gigerium, is an organ found in the digestive tract of some animals, including archosaurs (dinosaurs including birds, pterosaurs, crocodiles and alligators), earthworms, some gastropods, some fish, and some crustaceans.” 这段描述，可以推测鸡的胃囊在腺胃之后的消化道中，但具体的相对位置如在消化道的哪个部分，信息不足。\n\n答案：信息不足'\n  Doc IDs Type: <class 'list'>, Value: [9633, 3791, 197, 8446, 11972]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'where is the gizzard located on the chicken', 'answer': '根据提供的上下文信息，关于鸡的胃囊的具体位置描述不够明确。但是，可以知道鸡的胃囊是消化道的一部分，并且在“glandular stomach (腺胃) ”之后，“Then the food passes into the gizzard ( also known as the muscular stomach or ventriculus ) .” 这句话说明食物进入胃囊。结合“Gizzard of a chicken <P> The gizzard, also referred to as the ventriculus, gastric mill, and gigerium, is an organ found in the digestive tract of some animals, including archosaurs (dinosaurs including birds, pterosaurs, crocodiles and alligators), earthworms, some gastropods, some fish, and some crustaceans.” 这段描述，可以推测鸡的胃囊在腺胃之后的消化道中，但具体的相对位置如在消化道的哪个部分，信息不足。\\n\\n答案：信息不足', 'document_id': [9633, 3791, 197, 8446, 11972]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 33/1000 [04:49<2:52:37, 10.71s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- DEBUG: Preparing to write record 32 ---\n  Question Type: <class 'str'>, Value: 'what year did beyonce do the super bowl...'\n  Answer Type: <class 'str'>, Value: '根据提供的上下文信息，Beyoncé在Super Bowl XLVII halftime show中表演，Super Bowl XLVII是第47届超级碗，对应的年份是2013年（2013年2月3日）。因此，Beyoncé在2013年做了超级碗表演。\n\n答案：2013年'\n  Doc IDs Type: <class 'list'>, Value: [1170, 5870, 8851, 162, 10064]\n  Doc IDs List Length: 5\n  First Valid Doc ID Type: <class 'int'>\n  Output Record Dict: {'question': 'what year did beyonce do the super bowl', 'answer': '根据提供的上下文信息，Beyoncé在Super Bowl XLVII halftime show中表演，Super Bowl XLVII是第47届超级碗，对应的年份是2013年（2013年2月3日）。因此，Beyoncé在2013年做了超级碗表演。\\n\\n答案：2013年', 'document_id': [1170, 5870, 8851, 162, 10064]}\n--- End Debug Info ---\n","output_type":"stream"},{"name":"stderr","text":"Generating val predictions:   3%|▎         | 33/1000 [04:56<2:24:41,  8.98s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-082b4cca333c>\u001b[0m in \u001b[0;36m<cell line: 226>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;31m# d. 調用 LLM 生成答案\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                         \u001b[0mgenerated_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_answer_with_qwen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                         \u001b[0;31m# e. 格式化輸出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-082b4cca333c>\u001b[0m in \u001b[0;36mgenerate_answer_with_qwen\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"LLM 客戶端未初始化\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         response = llm_client.chat.completions.create(\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLM_MODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    827\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         )\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    994\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}